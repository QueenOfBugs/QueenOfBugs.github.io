<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Python 神经网络编程 &#x2013; 深度学习系列 神经网络入门</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="kamisama" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<style> .figure p {text-align: left;}</style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2020 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="h" href="./index.html"> UP </a>
 |
 <a accesskey="H" href="../index.html"> HOME </a>
</div><div id="content">
<h1 class="title">Python 神经网络编程 &#x2013; 深度学习系列 神经网络入门</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org25d310b">神经网络是如何工作的?</a>
<ul>
<li><a href="#org5a36035">尺有所长,寸有所短</a></li>
<li><a href="#orga8f4b03">一台简单的预测机</a></li>
<li><a href="#orgb477a12">分类器与预测器并无太大差别</a></li>
<li><a href="#orgaef8c9a">训练简单的分类器</a></li>
<li><a href="#orgae0b489">有时候一个分类器不足以求解问题</a></li>
<li><a href="#org5eee762">神经元&#x2013;大自然的计算器</a></li>
<li><a href="#org0a1a734">神经网络中追踪信号</a></li>
</ul>
</li>
<li><a href="#orgd5e443b">使用Python进行DIY</a>
<ul>
<li><a href="#org721b0c2">测试 emacs-jupyter</a></li>
<li><a href="#orgbfc0f10">开始制作神经网络</a>
<ul>
<li><a href="#org2fb5fda">框架代码</a></li>
<li><a href="#orge2b5ca9">初始化网络</a></li>
<li><a href="#org5cf7a9b">权重&#x2013;网络的核心</a></li>
<li><a href="#org60d469f">可选: 较复杂的权重</a></li>
<li><a href="#org0726683">查询网络</a></li>
<li><a href="#org5e9c5e8">目前为止的所有代码</a></li>
<li><a href="#orge100a91">训练网络</a></li>
<li><a href="#org3f387d9">完整的神经网络代码</a></li>
</ul>
</li>
<li><a href="#org1347e23">手写数字的数据集MNIST</a>
<ul>
<li><a href="#orgaef6384">准备MNIST训练数据</a></li>
<li><a href="#org9c9dac6">测试网络</a></li>
<li><a href="#org02f5a48">使用完整的数据集进行训练和测试</a></li>
<li><a href="#org7b1cb44">一些改进:调整学习率</a></li>
<li><a href="#orgac09740">一些改进:多次运行</a></li>
<li><a href="#org3f87dd1">改变网络形状</a></li>
<li><a href="#orge403155">大功告成!</a></li>
<li><a href="#orgc78bd80">最终代码:</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org2e80dc4">趣味盎然</a>
<ul>
<li><a href="#org1679c4f">自己的手写数字</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div id="outline-container-org25d310b" class="outline-2">
<h2 id="org25d310b">神经网络是如何工作的?</h2>
<div class="outline-text-2" id="text-org25d310b">
</div>
<div id="outline-container-org5a36035" class="outline-3">
<h3 id="org5a36035">尺有所长,寸有所短</h3>
<div class="outline-text-3" id="text-org5a36035">
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />
</colgroup>

<colgroup>
<col  class="org-left" />
</colgroup>

<colgroup>
<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Q</th>
<th scope="col" class="org-left">计算机</th>
<th scope="col" class="org-left">人类</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">快速对成千上万的大数字进行乘法运算</td>
<td class="org-left">简单</td>
<td class="org-left">困难</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-left">在一大群人的照片里查找面孔</td>
<td class="org-left">困难</td>
<td class="org-left">简单</td>
</tr>
</tbody>
</table>
<ul class="org-ul">
<li>有些人无,对传统计算机而言很容易,对人类却很难.例如,对百万个数字进行乘法运算.</li>
<li>另一方面,有些任务对传统的计算机而言很难,对人类却很容易.例如,从一群人的照片
中识别出面孔.</li>
</ul>
</div>
</div>

<div id="outline-container-orga8f4b03" class="outline-3">
<h3 id="orga8f4b03">一台简单的预测机</h3>
<div class="outline-text-3" id="text-orga8f4b03">
<p>
先从构建超级简单的机器开始了解神经网络中的学习过程.
</p>

<p>
想象一下,一台基本的机器,接受一个问题,做了一些"思考",并输出了一个答案.这和人
类识别照片时进行的操作一样,我们从眼睛输入图片,使用大脑分析场景,并得出场景中
有哪些物体的结论.那这台机器看起来可能是这样的:
</p>
<div class="org-src-container">
<pre class="src src-dot">digraph G{
rankdir = <span style="font-style: italic;">"LR"</span>;
input [label=<span style="font-style: italic;">"&#38382;&#39064;"</span>, shape=plaintext];
machine [label=<span style="font-style: italic;">"&#24605;&#32771;"</span>, shape=circle,width=0.75, height=0.75, fixedsize=true];
output [label=<span style="font-style: italic;">"&#31572;&#26696;"</span>, shape=plaintext];
input -&gt; machine -&gt; output;
}

</pre>
</div>

<p>
但是计算机并不能真正的思考,所以更确切的表述应该是这样的:
</p>
<div class="org-src-container">
<pre class="src src-dot">digraph G{
rankdir = <span style="font-style: italic;">"LR"</span>;
input [label=<span style="font-style: italic;">"&#36755;&#20837;"</span>, shape=plaintext];
machine [label=<span style="font-style: italic;">"&#27969;&#31243;\n(&#35745;&#31639;)"</span>, shape=circle,width=0.75, height=0.75, fixedsize=true];
output [label=<span style="font-style: italic;">"&#36755;&#20986;"</span>, shape=plaintext];
input -&gt; machine -&gt; output;
}
</pre>
</div>

<p>
以计算乘法 "3 × 4" 为例:
</p>

<div class="org-src-container">
<pre class="src src-dot">digraph G{
rankdir = <span style="font-style: italic;">"LR"</span>;
node [width=0.75, height=0.75, fixedsize=true];
input [label=<span style="font-style: italic;">"&#36755;&#20837;\n3&#215;4"</span>, shape=plaintext];
machine [label=<span style="font-style: italic;">"&#35745;&#31639;\n4+4+4"</span>, shape=circle,width=0.75, height=0.75, fixedsize=true];
output [label=<span style="font-style: italic;">"&#36755;&#20986;\n12"</span>, shape=plaintext];
input -&gt; machine -&gt; output;
}
</pre>
</div>

<p>
再增加点复杂度,在我们不知道千米和英里转换公式的前提下,要设计一台机器,能够根
据给定的数据将千米转化为英里:
    <img src="imgs/00045.gif" alt="00045.gif" />
</p>

<p>
现有条件:
</p>
<ul class="org-ul">
<li>我们知道千米和英里之间的关系是线性的.千米数加倍,表示相同距离的英里数也加
倍.即这种关系的形式应为: "英里=千米×C"</li>
<li><p>
我们有正确的千米和英里匹配的数据:
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<tbody>
<tr>
<td class="org-right">真实示例</td>
<td class="org-right">千米</td>
<td class="org-right">英里</td>
</tr>

<tr>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>

<tr>
<td class="org-right">2</td>
<td class="org-right">100</td>
<td class="org-right">62.137</td>
</tr>
</tbody>
</table></li>
</ul>

<p>
根据条件,我们计算出常数C, 就能将千米转换成英里.
我们先假设C=0.5, 然后让机器试试:
</p>


<div class="figure">
<p><img src="imgs/00141.gif" alt="00141.gif" />
</p>
</div>

<p>
这里我们得出了一个误差为 62.137-50 = 12.137 的结果.不算太差.
\(error = truth - calculated=62.137 - 50=12.137\)
</p>


<div class="figure">
<p><img src="imgs/00061.jpeg" alt="00061.jpeg" />
</p>
</div>


<p>
我们得出了一个不准确的结果,但是不要灰心,我们可以使用这个结果的误差来指导
我们得出更准确的C的猜测值.
误差值是12.137,由于公式是线性的,所以增加C就可以增加输出,减小误差.
那我们就将 C 从0.5增加到0.6, 得到英里=千米 × 0.6 = 60. 现在,误差值为
2.137,明显变得更小了.
</p>


<div class="figure">
<p><img src="imgs/00144.jpeg" alt="00144.jpeg" />
</p>
</div>

<p>
这里重点在于,我们是如何使用误差值的大小指导改变 C 的值.我们希望输出值从
50增大一点,所以稍微增加了C 的值.我们不需要使用代数方法计算出 C 需要改变
的确切量.我们可以继续使用刚刚的方法改进 C 值.
</p>
<blockquote>
<p>
如果实在是想要计算出确切的 C 值,那就要考虑一下,实际上遇到的更多的问题都
是无法使用一个简单的数学公式将输出和输入关联起来的.这就是我们需要使用诸
如神经网络这样相对成熟而复杂的方法的原因.
</p>
</blockquote>

<p>
那让我们继续调整 C 的值,既然0.6还是太小,我们可以将其调整到0.7:
</p>


<div class="figure">
<p><img src="imgs/00078.jpeg" alt="00078.jpeg" />
</p>
</div>

<p>
结果超过了正确答案,误差值为-7.863(真实值-计算值=62.137-70),这个负号告诉我们,C
的调整已经太过了,我们通常称参数的这种调整过度现象叫 <code>超调</code>.如此看来,0.6比0.7要
好,那我们可以就此结束,接受C=0.6 带来的小误差,但是,我们也可以使用一个更小的量,来
调整 C ,之前我们都是以 \(10^{-1}\) 的量级来调整,现在我们使用 \(10^{-2}\) 的量级
来调整,我们选 C = 0.61来看看结果如何?
</p>


<div class="figure">
<p><img src="imgs/00148.jpeg" alt="00148.jpeg" />
</p>
</div>

<p>
这次比前面的答案都要好,得到输出值61,误差值仅为1.137.
</p>

<p>
最后这次尝试能告诉我们的是,应该适度调整 C 的值,并且,随着输出值越来越接近正确
答案,即误差值越来越小,我们就不必使用大的调整,这样,就可以避免像先前那样 <code>超调</code> .
</p>

<p>
这么多尝试的步骤,都只是为了体现一个持续细化误差值的思想,事实上,我们都会建议
修正值取误差值的百分之几来进行调整.  直觉上,我们这样做是对的,因为,大的误差意
味着需要大的修正值,小误差意味着我们只需要小小地微调.
</p>

<p>
实际上,神经网络中"学习" 的核心过程就是刚才我们所做的事情. 我们训练机器,让其
输出值越来越接近正确的答案.
</p>

<p>
上面我们并未使用学校里解决数学问题时一步到位,精确求解问题,而是尝试直接得到答
案,并多次改进答案,这是一种非常不同的方法,有人称之为 <code>迭代</code> ,意思是持续地,一点
点地改进答案.
</p>

<ul class="org-ul">
<li>所有计算机系统都有输入和输出,并在输入和输出之间进行某种计算.神经网络也是一
样.</li>
<li>当我们不能精确知道事情如何运作(公式?)时,我们可以尝试使用模型来估计其运作方
式,在模型里,包括了我们可以调整的参数.就像我们不知道如何将千米转化为英里,我
们使用线性函数作为模型,并且使用可调节的梯度(斜率C)作为参数.</li>
<li>改进模型的 <code>一种</code> 好方法是, <span class="underline">基于模型和已知真实示例之间的比较,得到模型偏移的误
差值,再根据误差值调整参数</span> .</li>
</ul>
</div>
</div>

<div id="outline-container-orgb477a12" class="outline-3">
<h3 id="orgb477a12">分类器与预测器并无太大差别</h3>
<div class="outline-text-3" id="text-orgb477a12">
<p>
上面的简单机器接受了一个输入,并作出应有的预测,输出结果,我们将其称为预测器.根
据结果与已知真实示例进行比较所得出的误差,调整内部参数,使得预测更加精确.
现在,我们来看看两种小虫子的宽度和长度:
</p>


<div class="figure">
<p><img src="imgs/00095.jpeg" alt="00095.jpeg" />
</p>
</div>

<p>
红色的是毛虫,细而长,绿色的是瓢虫,宽而短.回想上面试图找到正确英里数的预测器,其
核心是一个可调节的线性函数.当绘制函数图像时,线型函数输出的是直线,可调参数 C
改变直线的斜率.
如果我们在这个虫子的长宽关系图里画一条直线,会发生什么呢?
</p>


<div class="figure">
<p><img src="imgs/00153.jpeg" alt="00153.jpeg" />
</p>
</div>

<p>
如果上图的直线能将毛虫和瓢虫划分开,那么这条直线就可以根据测量值来对未知的小虫
子进行分类.单由于一半的毛虫和瓢虫在分界线的同一侧,因此上图中的直线并不能做到
这一点.
我们再次调整斜率:
</p>


<div class="figure">
<p><img src="imgs/00155.jpeg" alt="00155.jpeg" />
</p>
</div>

<p>
这次的结果更差,完全没有区分.
那就再来试一次:
</p>


<div class="figure">
<p><img src="imgs/00157.jpeg" alt="00157.jpeg" />
</p>
</div>

<p>
这一条直线就好多了,可以整齐的将瓢虫和毛虫区分开来.那么,我们就可以将这条直线作
为小虫的分类器.
</p>

<p>
我们假设没有其他类型的虫子(只有毛虫和瓢虫),因为我们只是借此说明构建一台简单分
类器的思路.
</p>

<p>
设想下,当计算机使用机械臂抓起一只新的小虫子,测量其宽度和长度之后,就可以使用上
面最后一个分界线,来将虫子归类为毛虫或瓢虫.就像下图所示:
</p>


<div class="figure">
<p><img src="imgs/00158.gif" alt="00158.gif" />
</p>
</div>

<p>
我们已经看到了在简单的预测器里,使用线性函数来对未知的数据进行分类.
但是最重要的一点, <span class="underline">是我们如何得到正确的斜率呢?</span> 根据之前的思想,我们该如何改进分
界线,才能更好的划分这两种小虫呢?
</p>

<p>
这个问题的答案就是神经网络学习的核心. 让我们继续.
</p>
</div>
</div>

<div id="outline-container-orgaef8c9a" class="outline-3">
<h3 id="orgaef8c9a">训练简单的分类器</h3>
<div class="outline-text-3" id="text-orgaef8c9a">
<p>
上一节我们构思了一种线性分类器,能够正确分类瓢虫和毛虫,根据上节图示,我们可以观
察到,想要实现这样一种分类器,重点在于调整分界线的斜率,使其能够基于小虫的宽度和
长度将两组点划分开来.
</p>

<p>
我们不能上来就使用最前沿的先进数学理论,让我们先试着摸石头过河,这样我们可以更
好的了解数学.
</p>

<p>
我们也确实需要一些实例来训练.为了使这次练习简单点,下表只包含了两个实例:
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-right">实例</td>
<td class="org-right">宽度</td>
<td class="org-right">长度</td>
<td class="org-left">小虫</td>
</tr>

<tr>
<td class="org-right">1</td>
<td class="org-right">3.0</td>
<td class="org-right">1.0</td>
<td class="org-left">瓢虫</td>
</tr>

<tr>
<td class="org-right">2</td>
<td class="org-right">1.0</td>
<td class="org-right">3.0</td>
<td class="org-left">毛虫</td>
</tr>
</tbody>
</table>

<p>
这组数据是正确的.这些实例帮助我们调整分类函数的斜率. <span class="underline">用来训练预测器或分类器的
真实实例</span> ,我们称之为 <code>训练数据</code>
</p>

<p>
通过数字和表格不容易理解和感知数据,而可视化数据有助于我们做到这一点,因此让我
们先根据表格绘制出训练数据的图像:
</p>


<div class="figure">
<p><img src="imgs/00159.jpeg" alt="00159.jpeg" />
</p>
</div>

<p>
和千米转英里的例子类似,这里的分界线也是一条直线,我们进行类似的处理:
</p>

<p>
\(y=Ax\)
</p>

<p>
这条直线严格来说不是预测器,因此没有使用宽度和长度,而是用y,x. 这条直线是一条分
界线,这是一台分类器.
这里给出的直线形式也比直线的一般形式 \(y=Ax+b\) 要简单,这是因为这里刻意使分类小
虫的场景简化了.也就是我们假定在该场景直线不过座标原点的情况没有意义,毕竟这是
演示用,不是真实情景.
</p>

<p>
好了,我们可以开始操作了,之前我们就知道,参数A是控制直线的斜率的,较大的A对应较
大斜率.
</p>

<p>
让我们和之前一样先尝试给 A 赋值. 令 A = 0.25,即 \(y=0.25x\). 让我们绘制出他的图
像,来看看效果如何.
</p>


<div class="figure">
<p><img src="imgs/00160.jpeg" alt="00160.jpeg" />
</p>
</div>

<p>
不用计算就可以直接观察出,这条直线并不能将两种类型虫子区分开.毛虫和瓢虫代表的
点都处于直线之上.所以我们不能得出"如果小虫在直线之上,那么它就是毛虫"这样的结
论.
</p>

<p>
直觉告诉我们,我们需要将直线向上移动一点,但是我们不能这样做,我们不能通过观察就
画出一条适合的直线,毕竟我们最终的目的是找到一种可以重复的方法,通过计算机指令
来达到这个目标,计算机科学家将这一系列计算机指令称之为算法. 通俗的讲,我们是要
教会计算机去找出这条直线,而不是通过我们的"直觉"或观察来画出直线.
</p>

<p>
我们来观察第一个样本数据,宽为3.0,长为1.0 的瓢虫,如果我们使用尝试的第一个分界
线函数: \(y=0.25x\) ,我们会得到 \(y=0.25*3.0=0.75\), 表示对于宽3.0的小虫,长度应该
为0.75, 但是训练数据告诉我们,真实的长度应该为 1.0, 我们就知道这个数字太小了.现
在,我们有了误差了,根据前面的经验,我们可以根据误差来搞清楚如何调整参数A.
</p>

<p>
调整 A 之前,我们要先考虑下 y 应该是什么值,如果 y 为 1.0 那分界线会经过该点
\((x, y)= (3.0, 1.0)\) . 但是实际上,我们不希望出现这样的点,因为这条分界线是用来
给两种虫子分类的,我们希望直线能将两种虫子分开,而不是虫子代表的点出现在直线上.代
表虫子的点出现在直线上是训练一个给定小虫宽度,预测小虫长度的预测器时希望出现的
情况.
根据之前的图示,我们希望代表瓢虫的点能在直线下方,因此当 \(x=3.0\) 时,我们希望 y
能比1.0大一点, 那我们就先选取 y=1.1 来试一下.我们选1.1是因为它真的只比1.0大一
点,我们还可以选1.2或1.3,单我们不希望使用50,100这样的大数字,因为这会使直线在所
有虫子上方,导致这个分类器没有作用.
</p>

<p>
因此,当 \(x=3.0\) 时,我们的期望值 \(y=1.1\) 实际输出值 \(y=0.75\), 那么:
</p>

<p>
\(误差值E=(期望目标值-实际输出值)=1.1-0.75=0.35\)
</p>

<p>
我们先暂停一下,将误差值,期望值,和计算值的意义在图里表示出来:
</p>


<div class="figure">
<p><img src="imgs/00161.jpeg" alt="00161.jpeg" />
</p>
</div>


<p>
通过之前的实验,我们得出了要使用误差值来指导参数的调整的思想,之前的调整我们都
是凭直觉来进行较大或较小的调整.到了这里我们就要更近一步了,找出参数 A 的修正值
和误差值之间的关系,找到了二者的关联,我们就可以理解一个值如何影响另一个值了.
</p>

<p>
先从分类器的函数开始:
</p>

<p>
\(y=Ax\)
</p>

<p>
我们给定A的初始猜测值,计算出了错误的 y 的计算值,而实际正确的 y 值是基于样本数
据的.我们将正确的期望值 t 称之为目标值,为了得到 t 的值,我们需要稍微调整 A 的
值.数学上使用增量符号 \(\Delta\) 表示"微小的变化量". 那么 t 就可以表示成:
</p>

<p>
\(t=(A + \Delta{A})x\)
</p>

<p>
将其绘制成图像更容易理解:
</p>


<div class="figure">
<p><img src="imgs/00164.jpeg" alt="00164.jpeg" />
</p>
</div>

<p>
再次重申,误差值是期望值减去计算值计算出来的差值,也就是说 E 等于 t-y:
</p>

<p>
\(t-y=(A + \Delta{A})x - Ax=\Delta{A}x\)
</p>

<p>
所以, \(E=\Delta{A}x\).
</p>

<p>
既然我们想知道的是根据误差值如何指导参数的修正,那么将方程调整一下,得到:
</p>

<p>
\(\Delta{A}=E/x\)
</p>

<p>
有个这个表达式,我们就可以根据误差值E, 得到参数所需要的调整值 \(\Delta{A}\) 了.
</p>

<p>
Let's do it!
</p>

<p>
误差值为 0.35,x 为3.0,那么 \(\Delta{A}=E/x=0.35/3.0=0.1167\). 也就是说,当前的
A=0.25 需要加上 0.1167,那么修正后的 \(A=A + \Delta{A}=0.25+0.1167=0.3667\), 使
用这个 A 值,进行计算,得到的 y 值为1.1,这就是我们所期望的目标值.
</p>

<p>
现在我们完成了一个实例训练,让我们再根据下一个实例训练学习.此时,我们已知正确的
x=1.0,y=3.0, 更新后的A=0.3667.
</p>

<p>
当把 x=1.0 带入线性函数中去,我们观察结果.此时我们得到 y=0.3667, 与样本数据中
y=3.0 差了很多,根据先前相同的推理,我们希望直线不经过点,而是稍高或稍低于数据点,我
们将期望值设置为 \(y=2.9\). 这样,毛虫的样本点就位于直线上方,而不是直线上. 基于
此,误差值 \(E=2.9-0.3667=2.5333\).
</p>

<p>
和第一次相比,这个误差值更大,但仔细想想,到现在为止,我们都只使用了一个单一的样
本对线性函数进行训练,很明显,这使得直线每次都偏向于这个单一的样本.
</p>

<p>
那我们还是继续改进 A. \(\Delta{A}=E/x=2.5333/1.0=2.5333\), 那么
\(A=0.3667+2.5333=2.9\). 这意味着,对于 \(x=1.0\), 函数得出2.9的答案,这就是我们期
望的值.
</p>

<p>
让我们来看看已经取得的训练成果:
</p>


<div class="figure">
<p><img src="imgs/00165.jpeg" alt="00165.jpeg" />
</p>
</div>

<p>
发现了啥?直线并没有像我们希望的那样整齐的划分出毛虫和瓢虫.每次修正后的分界线
都偏向于修正时用到的样本数据点.这是为什么呢?
</p>

<p>
我们使用了所有训练数据样本来进行改进,但是我们上面的做法会让每一次改进的结果都
和最后一次训练数据样本点非常匹配,发现了吗?每次的改进都与其他的样本点无关,因为
我们根本就没用到其他训练数据样本点,有人可能会想,第二次 A 的初始值不就是第一次
的改进结果吗?那我们想想第一次 A 的初始值是怎么来的?我们只是随机选择了一个数,
而第二次的训练只不过是重复第一次的改进操作.也就是说,我们每次改进的结果,都抛弃
了之前的训练成果,而只是对最近一次实例数据进行了学习训练.
</p>

<p>
这个问题如何解决呢?
</p>

<p>
答案很简单! 机器学习里,一个重要的思路就是:我们要进行 <span class="underline">适度改进(moderate)</span>, 也就
是说,我们不要过于激烈地改进.
</p>

<p>
让我们使用 \(\Delta{A}\) 几分之一的变化值,而不是采用整个 \(\Delta{A}\), 每次都跳
跃到一个新的值.
</p>

<p>
使用这种方法,我们可以小心谨慎地向训练样本所只是的方向移动,并且在训练中保持了
先前训练迭代周期中所得到值的一部分. 千米和英里的转换的预测器中,我们就意识到了
这种有节制的调整,我们小心翼翼地调整参数,使其只是实际误差值的几分之几.
</p>

<p>
这种有节制的调整,还带来了一个非常强大的 "副作用".那就是在训练数据本身并不能保
证完全正确,或者包含现实世界中测量普遍会出现的错误或噪声时,这种适度改进可以抑
制这些错误或噪声的影响. <span class="underline">这种方法使得错误或噪声得到了调解和缓和</span>.
</p>

<p>
我们来改进下修正值的公式,原本的公式: \(\Delta{A}=E/x\), 改进后: \(\Delta{A}=L(E/x)\).
这种调节系数通常被称为 <code>学习率(learning rate)</code>, 这里,我们把它叫做L.
</p>

<p>
我们先挑一个 \(L=0.5\) 来做为一个合理的系数开始学习的过程,简单地说,就是每次更新
原更新值的一半.
</p>

<p>
使用添加了学习率的公式来重新训练分类器. 初始值 \(A=0.25\), 使用第一个训练样本点,
\(y=0.25 * 3.0=0.75\) ,期望值 \(t=1.1\), 误差值
\(E=1.1-0.75=0.35\). \(\Delta{A}=L(E/x)=0.5*(0.35/3.0)=0.0583\). 那么更新后的
\(A=0.25+0.0583=0.3083\).
尝试使用新的 A 值来计算下第一个训练样本点, \(x=3.0,y=0.3083 * 3.0=0.9250\). 现
在,这个值小于1,因此,直线落在样本点错误的一边,但是,如果把这视为后续众多调整步
骤中的第一步,则这个结果并不算太差,因为和初始的直线相比,这条直线确实是向正确的
方向在移动.
</p>

<p>
使用新的 A 值来继续训练,第二个样本点, \(y=0.3083* 1.0 = 0.3083\), 期望值
\(t=2.9\), 误差值
\(E=t-y=2.9-0.3083=2.5917\). \(\Delta{A}=L(E/x)=0.5*(2.5917/1.0)=1.2958\). 那么
第二个更新后的 \(A=0.3083+1.2958=1.6042\).
</p>


<ul class="org-ul">
<li>TODO 我们再次观察添加了学习率之后进行的训练结果的图像:</li>
</ul>

<p>
即使使用两个简单的训练样本,利用带有调节学习速率的一种相对简单的改进方法,我们
也迅速地得到了一条很好的分界线 \(y=Ax,A=1.6042\)
</p>

<ul class="org-ul">
<li>我们知道了在何种程度上调整斜率,可以消除输出误差.</li>
<li>使用朴素的调整方法会导致改进后的模型只与最后一次训练的样本最匹配,忽略了所
有以前的训练样本.解决问题就是使用 <span class="underline">学习率</span>,调节改进速率,这样就可以使单一的训
练样本不能主导整个学习过程.</li>
<li>真实世界的训练样本可能充满噪声或包含错误,适度改进(moderate) 可以限制这些错
误样本的影响.</li>
</ul>
</div>
</div>

<div id="outline-container-orgae0b489" class="outline-3">
<h3 id="orgae0b489">有时候一个分类器不足以求解问题</h3>
<div class="outline-text-3" id="text-orgae0b489">
<ul class="org-ul">
<li>如果数据本身不是由单一线性过程支配,那么一个简单的线性分类器不能对数据进行划
分.例如由逻辑 XOR 运算符支配的数据.</li>
<li>解决方法就是使用多个线性分类器划分由单一直线无法分离的数据.</li>
</ul>
</div>
</div>

<div id="outline-container-org5eee762" class="outline-3">
<h3 id="org5eee762">神经元&#x2013;大自然的计算器</h3>
<div class="outline-text-3" id="text-org5eee762">
<p>
计算机有大量电子计算元件,巨大存储空间,并且计算机的运行频率比生物大脑要快得多,
但即使是像鸽子一样小的大脑,能力也远大于电子计算机.
传统计算机的按照严格的串行顺序,相当准确具体地处理数据.对计算机来说,不存在模糊
性和不确定性.另一方面,生物大脑虽然看起来运行得慢得多,但似乎是以并行方式处理信
号, <span class="underline">模糊性</span> 是其计算的一种特征.
神经元图像:
<img src="imgs/_gallerySharetemp.png" alt="_gallerySharetemp.png" />
</p>

<p>
神经元有各种形式,但所有神经元都是将电信号从一端传输到另一端,沿着轴突,将电信号
从树突传到树突.这些电信号从一个神经元传送到另一个神经元,这就是身体感知光,声,
热等信号的机制.专门的感觉神经元的信号沿着神经系统,传输到大脑,而大脑本身也主要
是由神经元构成的.
</p>

<p>
观察表明,神经元不会立即反应,而是会抑制输出,指导输出增强,强达到可以触发输出.
下图说明了这种思想,只有输出超过了阈值,足够接通电路,才会产生输出信号.
</p>


<div class="figure">
<p><img src="imgs/00031.jpeg" alt="00031.jpeg" />
</p>
</div>

<p>
虽然这个函数接受了输入信号,产生了输出信号,但是我们要将称为激活函数的阈值考虑
在内.在数学上,有许多激活函数可以达到这样的效果,一个简单的阶跃函数就可以实现这
种效果.
</p>


<div class="figure">
<p><img src="imgs/00182.gif" alt="00182.gif" />
</p>
</div>

<p>
在值较小的情况下,输出为0,一旦达到阈值,输出就一跃而起,具有这种行为的人工神经元
就像一个真正的生物神经元.科学家所使用的属于非常形象地描述了这种行为,他们说,输
入值达到阈值时,神经元就激发了.
</p>

<p>
我们可以改进阶跃函数,下面是 S 形函数(称为S函数:sigmoid function)的图像,比起阶
跃函数,S 形函数相对平滑,使得函数更自然,更接近现实,自然界很少有冰冷尖锐的边缘
<img src="imgs/00183.gif" alt="00183.gif" />
</p>

<p>
我们会使用这种平滑的 S 形函数制作神经网络.虽然人工只能研究员也使用其他外形类
似的函数,但 S 函数简单,并且事实上非常常见.
</p>

<p>
S 函数,有时也称为逻辑函数,其表达式为:
</p>

<p>
\(y = \frac{1}{1 + e^{-x}}\)
</p>

<p>
我们使用S 函数而不使用其他 S 形函数的另一个重要原因就是,这个 S 函数比其他 S
形函数计算起来要容易的多.
</p>

<p>
下面我们来构思如何建模人工神经.
</p>

<p>
生物神经元可以接受许多输入,而不仅仅是一个输入.
</p>

<p>
那模拟时对于这些输入,我们该怎么做呢? 我们只需要对他们进行相加,得到最终结果的
综合,作为 S 函数的输入,然后输出结果.
下图说明了这种组合输入,然后对最终输入总和使用阈值的思路.
</p>


<div class="figure">
<p><img src="imgs/00185.jpeg" alt="00185.jpeg" />
</p>
</div>

<p>
树突收集电信号,将其组合形成更强的电信号,如果信号足够强,超过阈值,神经元就会发
射信号,沿着轴突到达终端,将信号传递给下一个神经元的树突.根据上面的思路,如果组
合信号不够大,那么 S 函数的效果是抑制输出信号.如果总和x 足够大,S 函数的效果就
是激发神经元,有趣的是,如果只有一个输入足够大,其他信号都很小,那也足够激发神经
元;如果单一信号都不是很大,但组合的信号足够大,神经元也能激发.
下图是若干神经元的链接示意图:
</p>


<div class="figure">
<p><img src="imgs/00187.jpeg" alt="00187.jpeg" />
</p>
</div>

<p>
每个神经元接受来自其之前多个神经元的输入,如果神经元被激发,它也提供信号给更多
的神经元.
</p>

<p>
这种自然形式复制到人造模型的一种方法,就是构建多层神经元,每一层中的神经元都与
其前后层的神经元相互链接:
</p>


<div class="figure">
<p><img src="imgs/00189.jpeg" alt="00189.jpeg" />
</p>
</div>


<p>
神经元的链接已经模拟出来了,那如何模拟神经网络的学习能力呢?针对训练的样本,我们
应该如何做出反应呢?有没有和先前线性分类器中的斜率类似的参数让我们来调整呢?
</p>

<p>
最明显的一点是调整节点之间的链接强度,,一个节点内,我们可以调整输入的总和或者S
函数的形状,但是比起调整函数的形状,调整节点之间的链接强度更为简单.
</p>

<p>
如果简单的方法可以工作,那请坚持简单的方法!
</p>

<p>
下面再次显示了链接的节点,但是为每隔链接添加了相关权重,较小的权重将弱化信号,较
大的权重将放大信号.
</p>


<div class="figure">
<p><img src="imgs/00191.jpeg" alt="00191.jpeg" />
</p>
</div>

<p>
我们将前后层的每一个神经元根所有其他层神经元相互链接,是因为这种方式容易编码成
计算机指令,并且,不必要的链接会在学习过程中被弱化(权重趋向于0),所以,多级个冗余
链接,无伤大雅.
这种弱化的来由是我们会在神经网络的训练过程中,神经网络会通过调整权重改进输出,
一些链接权重可能会变成0或接近0,这样的链接意味着它对神经网络的贡献为0,实际上这
种链接在网络中几乎不起作用,等同于被断开了.
</p>
</div>
</div>

<div id="outline-container-org0a1a734" class="outline-3">
<h3 id="org0a1a734">神经网络中追踪信号</h3>
<div class="outline-text-3" id="text-org0a1a734">
<p>
我们使用每层两个的神经元的较小神经网络来演示下神经网络是如何工作的:
</p>


<div class="figure">
<p><img src="imgs/00195.jpeg" alt="00195.jpeg" />
</p>
</div>

<p>
我们假设下神经网络的初始输入和权重:
</p>


<div class="figure">
<p><img src="imgs/00198.jpeg" alt="00198.jpeg" />
</p>
</div>

<p>
第一层节点是输入层,这一层不做其他事情,只是表示输入信号.输入节点不对输入信号应
用激活函数.这没有特殊原因,历史就是这样规定的,神经网络第一层是输入层,这层所做
事情就是表示输入,仅此而已.
</p>

<p>
第二层就需要进行一些计算.这一层每个节点,我们需要计算出组合输入.回顾下 S 函数:
</p>

<p>
\((y=\frac{1}{1 + e^{-x}})\)
</p>

<p>
x 表示一个节点的输入,由于一个节点有上一层每个节点的多个信号传入,因此,需要将这
些信号组合起来形成这个节点的输入信号,上层节点的输入信号传递到这个节点的过程收
到链接权重的调节.下图就是我们先前构思的图,但是现在加上了使用权重调节信号的过
程:
</p>


<div class="figure">
<p><img src="imgs/00200.jpeg" alt="00200.jpeg" />
</p>
</div>


<p>
目光汇聚在第二层第一个节点.第一层有两个节点连接到它,这些输入节点具有1.0和0.5
的原始值,与第二层第一个节点的链接对应为0.9和0.3的权重,因此,组合后的输入信号:
</p>

<p>
\(x=( 第一个节点的输出* 链接权重) + (第二个节点的输出 * 链接权重)\)
</p>

<p>
\(x=(1.0*0.9) + (0.5*0.3)\)
</p>
</div>
</div>
</div>
<div id="outline-container-orgd5e443b" class="outline-2">
<h2 id="orgd5e443b">使用Python进行DIY</h2>
<div class="outline-text-2" id="text-orgd5e443b">
</div>
<div id="outline-container-org721b0c2" class="outline-3">
<h3 id="org721b0c2">测试 emacs-jupyter</h3>
<div class="outline-text-3" id="text-org721b0c2">
<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold;">import</span> sys
<span style="font-weight: bold;">print</span>(sys.version)
</pre>
</div>


<div class="org-src-container">
<pre class="src src-jupyter-python">#import matplotlib.pyplot
#import numpy
import sys
print(sys.version)
#a = numpy.zeros([3,2])
#a[0,0]=1
#a[0,1]=2
#a[1,0]=9
#a[2,1]=12
#matplotlib.pyplot.imshow(a,interpolation="nearest")
#print(a)
</pre>
</div>
</div>
</div>

<div id="outline-container-orgbfc0f10" class="outline-3">
<h3 id="orgbfc0f10">开始制作神经网络</h3>
<div class="outline-text-3" id="text-orgbfc0f10">
<p>
从小处开始,然后让程序慢慢长大,这好似构建中等复杂度计算机代码的一种明智的方式.
</p>
</div>

<div id="outline-container-org2fb5fda" class="outline-4">
<h4 id="org2fb5fda">框架代码</h4>
<div class="outline-text-4" id="text-org2fb5fda">
<p>
神经网络类应该至少有三个函数:
</p>
<dl class="org-dl">
<dt>初始化函数</dt><dd>设定输入层节点,隐藏层节点和输出层节点的数量.</dd>
<dt>训练</dt><dd>学习给定训练集样本后,优化权重.</dd>
<dt>查询</dt><dd>给定输入,从输出节点给出答案.</dd>
</dl>

<p>
代码框架:
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">neural network class definition</span>
<span style="font-weight: bold;">class</span> <span style="font-weight: bold; text-decoration: underline;">neuralNetwork</span>:
    <span style="font-weight: bold; font-style: italic;">#</span><span style="font-weight: bold; font-style: italic;">initialize the neutral network.</span>
    <span style="font-weight: bold;">def</span> <span style="font-weight: bold;">__init__</span>():
        <span style="font-weight: bold;">pass</span>
    <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">train the neural network</span>
    <span style="font-weight: bold;">def</span> <span style="font-weight: bold;">train</span>():
        <span style="font-weight: bold;">pass</span>
    <span style="font-weight: bold; font-style: italic;">#</span><span style="font-weight: bold; font-style: italic;">query the neural network</span>
    <span style="font-weight: bold;">def</span> <span style="font-weight: bold;">query</span>():
        <span style="font-weight: bold;">pass</span>
</pre>
</div>
</div>
</div>

<div id="outline-container-orge2b5ca9" class="outline-4">
<h4 id="orge2b5ca9">初始化网络</h4>
<div class="outline-text-4" id="text-orge2b5ca9">
<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">initialize the neutral network</span>
<span style="font-weight: bold;">def</span> <span style="font-weight: bold;">__init__</span>(<span style="font-weight: bold;">self</span>, inputnodes, hiddennodes, outputnodes, learningrate):
    <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">set number of nodes in each input,hidden,output layer</span>
    <span style="font-weight: bold;">self</span>.inodes = inputnodes
    <span style="font-weight: bold;">self</span>.hnodes = hiddennodes
    <span style="font-weight: bold;">self</span>.onodes = outputnodes
    <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">learning rate</span>
    <span style="font-weight: bold;">self</span>.lr = learningrate
    <span style="font-weight: bold;">pass</span>
</pre>
</div>

<p>
使用定义的神经网络类,创建每层 3个节点, 学习率为0.5 的小型神经网络对象.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">number of input hidden and output nodes</span>
<span style="font-weight: bold; font-style: italic;">input_nodes</span> = 3
<span style="font-weight: bold; font-style: italic;">hidden_nodes</span> = 3
<span style="font-weight: bold; font-style: italic;">output_nodes</span> = 3

<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">learning rate is 0.5</span>
<span style="font-weight: bold; font-style: italic;">learning_rate</span> = 0.5

<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">create instance of neutral network</span>
<span style="font-weight: bold; font-style: italic;">n</span> = neuralNetwork(input_nodes, hiddennodes, output_nodes, learning_rate)
</pre>
</div>
</div>
</div>

<div id="outline-container-org5cf7a9b" class="outline-4">
<h4 id="org5cf7a9b">权重&#x2013;网络的核心</h4>
<div class="outline-text-4" id="text-org5cf7a9b">
<p>
前面我们知道使用矩阵可以简明地表达权重.所以我们可以创建:
</p>
<ul class="org-ul">
<li>输入层和隐藏层之间的链接权重矩阵 \(W_{input_hidden}\), 大小为hidden_nodes 乘
以 output_nodes.</li>
<li>隐藏层和输出层之间的链接权重矩阵为 \(W_{hidden_output}\), 大小为
hidden_nodes 乘以 output_nodes.</li>
</ul>


<p>
链接权重的初始值应该较小,并且是随机的.
使用 <code>numpy</code> 的函数生成一个数组,数组重元素为 0-1 的随机值,数组大小为row乘以
columns.
</p>
<div class="org-src-container">
<pre class="src src-python">numpy.random.rand(rows,columns)
</pre>
</div>
<p>
numpy 函数的使用手册可以在互联网上查找.
</p>

<p>
下面显示了生成3×3的numpy数组.数组中每个值都是 0-1的随机值.
</p>
<div class="org-src-container">
<pre class="src src-jupyter-python">import numpy
numpy.random.rand(3, 3)
</pre>
</div>

<pre class="example">
array([[0.89201918, 0.37893307, 0.3509575 ],
       [0.81494134, 0.40866014, 0.81253467],
       [0.27823627, 0.26021284, 0.31133589]])
</pre>


<p>
上面没有考虑权重可以为正数,也可以为负数的情况,权重范围可以在 -1.0 到 +1.0 之
间.为了简单起见,我们在上面数组的每个值里都减去0.5,这样,让数组中每个值都是-0.5到
+0.5之间的随机值
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python">import numpy
numpy.random.rand(3, 3) - 0.5
</pre>
</div>

<pre class="example">
array([[-0.38037314, -0.47892486, -0.13405653],
       [ 0.44163758,  0.2625298 ,  0.19114613],
       [-0.18986603, -0.30814275, -0.13159751]])
</pre>


<p>
权重是神经网络的固有部分.不会随着函数的调用结束消失,所以权重也必须是初始化的
一部分,并且可以使用其他函数来访问(训练函数和查询函数).
下面是创建链接权重矩阵的实例代码:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">link weight matrices, with and who weights inside the arrays are  w_i_j, where link is from node i to node j in the next layer</span>
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">w11 w21</span>
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">w12 w22 etc.</span>
<span style="font-weight: bold;">self</span>.wih = (numpy.random.rand(<span style="font-weight: bold;">self</span>.hnodes, <span style="font-weight: bold;">self</span>.inodes) - 0.5)
<span style="font-weight: bold;">self</span>.who = (numpy.random.rand(<span style="font-weight: bold;">self</span>.hnodes, <span style="font-weight: bold;">self</span>.onodes) - 0.5)
</pre>
</div>

<p>
到这里，我们就准备好了神经网络的核心,链接权重矩阵.
</p>
</div>
</div>

<div id="outline-container-org60d469f" class="outline-4">
<h4 id="org60d469f">可选: 较复杂的权重</h4>
<div class="outline-text-4" id="text-org60d469f">
<p>
我们可以选择一种简单却流行的优化初始权重的方式.
</p>

<p>
有些人更喜欢使用稍微复杂的方式来创建初始随机权重.他们使用正态概率分布采样权
重,其中,平均值为0,标准方差为节点传入链接数目的开放,即 \(\frac{1}{\sqrt{传入
    链接数目}}\).
numpy 库可以帮助我们实现这点. <code>numpy.random.normal()</code> 函数可以帮助我们以正态分
布的方式采样.因为我们需要的是随机矩阵,而不是单一数字,因此采用分布中心值,标准
方差和numpy 数组的大小最为参数.
此外,我们使用下一层的节点数的开方最为标准方差来初始化权重.
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold;">self</span>.wih = numpy.random.normal(0.0, <span style="font-weight: bold;">pow</span>(<span style="font-weight: bold;">self</span>.hnodes, -5), (<span style="font-weight: bold;">self</span>.hnodes, <span style="font-weight: bold;">self</span>.inodes))
<span style="font-weight: bold;">self</span>.who = numpy.random.normal(0.0, <span style="font-weight: bold;">pow</span>(<span style="font-weight: bold;">self</span>.onodes, -5), (<span style="font-weight: bold;">self</span>.hnodes, <span style="font-weight: bold;">self</span>.onodes))
</pre>
</div>
<p>
normal 函数第一个参数是正态分布的中心,第二个参数是标准方差,第三个是数组大小.
</p>
</div>
</div>

<div id="outline-container-org0726683" class="outline-4">
<h4 id="org0726683">查询网络</h4>
<div class="outline-text-4" id="text-org0726683">
<p>
根据逻辑,我们因该开始填充 train() 函数.让我们先推迟一下这一步,来先编写简单的
query() 函数,这会给我们更多时间来逐步建立信心,获得使用Python 和神经网络对象
内部权重矩阵的实践经验.
query() 函数接受神经网络的输入,返回网络的输出.
也就是说,我们要传递来自输入层节点的输入信号,通过隐藏层,最后从输出层输出.当信
号传送到给定的隐藏节点或输出层节点时,我们使用链接权重调节信号,并应用S激活函
数来抑制这些节点的信号.
如果有很多节点,那我们就要为每一个节点写出Python 代码,来进行权重调节,信号求和,
应用激活函数.节点越多,代码越多.
好在我们之前就讨论过使用矩阵形式写出这些指令,例如输入层和隐藏层之间的链接权
重矩阵与输入矩阵相乘,得到隐藏层节点的输入信号:
\(X_{hidden} = W_{input\_hidden}\cdot{I}\)
这样可以更容易书写,而Python 也可以理解矩阵.
下面是python代码完成上面的矩阵乘法:
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold; font-style: italic;">hidden_inputs</span> = numpy.dot(<span style="font-weight: bold;">self</span>.wih, inputs)
</pre>
</div>
<p>
将激活函数应用到隐藏层节点的输入信号上,得到隐藏层的输出信号矩阵:
\({O}_{hidden} = sigmoid({X}_{hidden})\)
激活函数 sigmoid 在python Scipy 库里已经实现,但是函数名是 expit.
导入 expit():
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold;">from</span> scipy.special <span style="font-weight: bold;">import</span> expit
</pre>
</div>
<p>
由于我们可能会对激活函数进行调整或者完全改变激活函数,因此,当神经网络对象初始
化时,在神经网络对象内部只定义一次激活函数是有道理的,不然我们就要在代码的多处
进行修改了.
下面定义了希望使用的激活函数:
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold;">self</span>.activation_function = <span style="font-weight: bold;">lambda</span> x: expit(x)
</pre>
</div>
<p>
这里解释下lambda,lambda 就是Python 的语法糖,是定义函数的一种简写形式,这里就是
定义了一个函数,接受参数 x ,返回 expit(x). 用 lambda 创建的函数是没有名字的,所
以一般叫这种函数为匿名函数,但是这里给它分配了一个名字
self.activation_function, 所以我们可以通过 self.activation_function() 来调用
这个函数.
回到给隐藏层节点的输入信号应用激活函数生成隐藏层的输出信号:
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold; font-style: italic;">hidden_outputs</span> = <span style="font-weight: bold;">self</span>.activation_function(hidden_inputs)
</pre>
</div>
<p>
信号成功到达隐藏层并生成输出信号,而信号从隐藏层传递到最终输出层的过程和信号从
输入层传递到隐藏层没有本质上的区别,代码也非常类似.
下面代码总结了如何计算隐藏层信号和输出层信号.
</p>
<div class="org-src-container">
<pre class="src src-python">
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">calculate signals into hidden layer</span>
<span style="font-weight: bold; font-style: italic;">hidden_inputs</span> = numpy.dot(<span style="font-weight: bold;">self</span>.wih, inputs)
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">calculate signals emerging from hidden layer</span>
<span style="font-weight: bold; font-style: italic;">hidden_outputs</span> = <span style="font-weight: bold;">self</span>.activation_function(hidden_inputs)

<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">calculate signals into final output layer</span>
<span style="font-weight: bold; font-style: italic;">final_inputs</span> = numpy.dot(<span style="font-weight: bold;">self</span>.who, hidden_outputs)
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">calculate the signals emerging from final output layer</span>
<span style="font-weight: bold; font-style: italic;">final_outputs</span> = <span style="font-weight: bold;">self</span>.activation_function(final_inputs)

</pre>
</div>
</div>
</div>

<div id="outline-container-org5e9c5e8" class="outline-4">
<h4 id="org5e9c5e8">目前为止的所有代码</h4>
<div class="outline-text-4" id="text-org5e9c5e8">
<p>
让我们检查一下已经构建好的代码:
</p>
<div class="org-src-container">
<pre class="src src-jupyter-python"># neural network class definition
import numpy
from scipy.special import expit
class neuralNetwork:
    #initialize the neutral network.
    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):
	# set number of nodes in each input,hidden,output layer
	self.inodes = inputnodes
	self.hnodes = hiddennodes
	self.onodes = outputnodes
	# learning rate
	self.lr = learningrate
	self.wih = numpy.random.normal(0.0, pow(self.hnodes, -0.5), (self.hnodes, self.inodes))
	self.who = numpy.random.normal(0.0, pow(self.onodes, -0.5), (self.onodes, self.hnodes))
	self.activation_function = lambda x: expit(x)
	pass
    # train the neural network
    def train():
	pass
    #query the neural network
    def query(self, inputs_list):
	# convert inputs list to 2d array
	inputs = numpy.array(inputs_list, ndmin=2).T
	# calculate signals into hidden layer
	hidden_inputs = numpy.dot(self.wih, inputs)
	# calculate the signals emerging form hidden layer
	hidden_outputs = self.activation_function(hidden_inputs)
	# calculate the signals into final output layer
	final_inputs = numpy.dot(self.who, hidden_outputs)
	# calculate the signals emerging from final output layer
	final_outputs = self.activation_function(final_inputs)
	return final_outputs
input_nodes = 3
hidden_nodes = 3
output_nodes= 3
learning_rate = 0.3
n = neuralNetwork(input_nodes, hidden_nodes,output_nodes, learning_rate)
print(n.query([1.0, 0.5, -1.5]))
</pre>
</div>

<p>
最后几行代码是测试查询功能是否好使.输入的是一个列表,Python 将列表写在方括号
里,输出也是数字列表,由于没有训练网络,这个输出没有实际意义,但这表明代码没有出
错.
</p>
</div>
</div>

<div id="outline-container-orge100a91" class="outline-4">
<h4 id="orge100a91">训练网络</h4>
<div class="outline-text-4" id="text-orge100a91">
<p>
现在来填充较为复杂的训练任务,训练任务分为两个部分:
</p>
<ul class="org-ul">
<li>针对给定训练样本计算输出,这与刚刚在 query() 函数上所做的没有区别.</li>
<li>将计算得到的输出和所需输出对比,使用差值来指导网络权重的更新.</li>
</ul>

<p>
第一部分就是查询函数的代码:
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold;">def</span> <span style="font-weight: bold;">train</span>(<span style="font-weight: bold;">self</span>, input_list, targets_list):
    <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">convert input list to 2d array</span>
    <span style="font-weight: bold; font-style: italic;">inputs</span> = numpy.array(input_list, ndmin=2).T
    <span style="font-weight: bold; font-style: italic;">targets</span> = numpy.array(targets_list, ndmin=2).T
    <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">calculate signals into hidden layer</span>
    <span style="font-weight: bold; font-style: italic;">hidden_inputs</span> = numpy.dot(<span style="font-weight: bold;">self</span>.wih, inputs)
    <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">calculate signals emerging from hidden layer</span>
    <span style="font-weight: bold; font-style: italic;">hidden_outputs</span> = <span style="font-weight: bold;">self</span>.activation_function(hidden_inputs)

    <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">calculate signals into final output layer</span>
    <span style="font-weight: bold; font-style: italic;">final_inputs</span> = numpy.dot(<span style="font-weight: bold;">self</span>.who, hidden_inputs)
    <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">calculate signals emerging from final output layer</span>
    <span style="font-weight: bold; font-style: italic;">final_outputs</span> = <span style="font-weight: bold;">self</span>.activation_function(final_inputs)
    <span style="font-weight: bold;">pass</span>
</pre>
</div>
<p>
我们使用完全相同的方式从输入层到输出层传递信号,所以这段代码和 query() 函数中
的代码几乎完全一样.
因为需要使用期望值或目标答案的样本来训练网络,因此有一个额外的参数,即 targets_list.
接下来我们就需要基于计算输出和目标输出之间的误差,改进权重了.
首先,要计算误差,这个值等于预期值与计算值的输出值之差.这个差可以将矩阵
targets_list 和矩阵 final_inputs 中每个对应元素相减得到.Python 代码非常简单:
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">error is the (target - actual)</span>
<span style="font-weight: bold; font-style: italic;">output_errors</span> = targets - final_outputs
</pre>
</div>
<p>
接下来再计算出隐藏层节点反向传播的误差.这个过程的矩阵形式是:
\({errors}_{hidden} = {{weights}^{T}}_{hidden\_output} \cdot {errors}_{output}\)
使用 Python 代码计算十分简单:
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">hidden layer error is the output_errors, split by weights, recombined at hidden nodes</span>
<span style="font-weight: bold; font-style: italic;">hidden_errors</span> = numpy.dot(<span style="font-weight: bold;">self</span>.who.T, output_errors)
</pre>
</div>
<p>
这样我们就有了所需要的一切误差值矩阵.下面可以着手优化链接权重.
隐藏层和最终层的权重使用 output_errors 优化,输入层和隐藏层之间的权重,使用
hidden_errors 进行优化.
首先,回顾一下用于更新节点 j 和下一层节点 k 之间链接权重的矩阵形式的表达式:
\(\Delta{W_{j,k}} =\alpha * E_{k} * sigmoid(\sum_{j}{W}_{j,k}\cdot{O_{j}}) *
    (1 - sigmoid(\sum_{j}{W}_{j,k}\cdot{O_{j}})) \cdot{O_j^T}\)
α 是学习率,sigmoid 是激活函数 <code>*</code> 是正常乘法, \(\cdot\) 是矩阵点乘, 来自上一层的
输出矩阵被转置了.
Python 代码中,转置很容易实现.
首先为隐藏层和最终层之间的权重进行编码:
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">update the weights for the links between the hidden and output layers</span>
<span style="font-weight: bold;">self</span>.who += <span style="font-weight: bold;">self</span>.lr * numpu.dot((output_errors * final_outputs * (1.0 - final_outputs)), numpy.transpose(hidden_outputs))
</pre>
</div>
<p>
输入层和隐藏层之间的权重代码也类似,下面是两个权重的代码:
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">update the weights for the links between the hidden and output layers</span>
<span style="font-weight: bold;">self</span>.who += <span style="font-weight: bold;">self</span>.lr * numpu.dot((output_errors * final_outputs * (1.0 - final_outputs)), numpy.transpose(hidden_outputs))
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">update the weights for the links between the hidden and output layers</span>
<span style="font-weight: bold;">self</span>.wih += <span style="font-weight: bold;">self</span>.lr * numpu.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), numpy.transpose(inputs))
</pre>
</div>
<p>
ok 完成了更新矩阵的计算.
</p>
</div>
</div>

<div id="outline-container-org3f387d9" class="outline-4">
<h4 id="org3f387d9">完整的神经网络代码</h4>
<div class="outline-text-4" id="text-org3f387d9">
<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">neural network class definition</span>
<span style="font-weight: bold;">import</span> numpy
<span style="font-weight: bold;">from</span> scipy.special <span style="font-weight: bold;">import</span> expit
<span style="font-weight: bold;">class</span> <span style="font-weight: bold; text-decoration: underline;">neuralNetwork</span>:

    <span style="font-weight: bold; font-style: italic;">#</span><span style="font-weight: bold; font-style: italic;">initialize the neutral network.</span>
    <span style="font-weight: bold;">def</span> <span style="font-weight: bold;">__init__</span>(<span style="font-weight: bold;">self</span>, inputnodes, hiddennodes, outputnodes, learningrate):
        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">set number of nodes in each input,hidden,output layer</span>
        <span style="font-weight: bold;">self</span>.inodes = inputnodes
        <span style="font-weight: bold;">self</span>.hnodes = hiddennodes
        <span style="font-weight: bold;">self</span>.onodes = outputnodes
        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">learning rate</span>
        <span style="font-weight: bold;">self</span>.lr = learningrate
        <span style="font-weight: bold;">self</span>.wih = numpy.random.normal(0.0, <span style="font-weight: bold;">pow</span>(<span style="font-weight: bold;">self</span>.hnodes, -0.5), (<span style="font-weight: bold;">self</span>.hnodes, <span style="font-weight: bold;">self</span>.inodes))
        <span style="font-weight: bold;">self</span>.who = numpy.random.normal(0.0, <span style="font-weight: bold;">pow</span>(<span style="font-weight: bold;">self</span>.onodes, -0.5), (<span style="font-weight: bold;">self</span>.onodes, <span style="font-weight: bold;">self</span>.hnodes))
        <span style="font-weight: bold;">self</span>.activation_function = <span style="font-weight: bold;">lambda</span> x: expit(x)
        <span style="font-weight: bold;">pass</span>
    <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">train the neural network</span>
    <span style="font-weight: bold;">def</span> <span style="font-weight: bold;">train</span>(<span style="font-weight: bold;">self</span>, input_list, targets_list):
        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">convert input list to 2d array</span>
        <span style="font-weight: bold; font-style: italic;">inputs</span> = numpy.array(input_list, ndmin=2).T
        <span style="font-weight: bold; font-style: italic;">targets</span> = numpy.array(targets_list, ndmin=2).T
        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">calculate signals into hidden layer</span>
        <span style="font-weight: bold; font-style: italic;">hidden_inputs</span> = numpy.dot(<span style="font-weight: bold;">self</span>.wih, inputs)
        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">calculate signals emerging from hidden layer</span>
        <span style="font-weight: bold; font-style: italic;">hidden_outputs</span> = <span style="font-weight: bold;">self</span>.activation_function(hidden_inputs)
        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">calculate signals into final output layer</span>
        <span style="font-weight: bold; font-style: italic;">final_inputs</span> = numpy.dot(<span style="font-weight: bold;">self</span>.who, hidden_inputs)
        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">calculate signals emerging from final output layer</span>
        <span style="font-weight: bold; font-style: italic;">final_outputs</span> = <span style="font-weight: bold;">self</span>.activation_function(final_inputs)
        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">update the weights for the links between the hidden and output layers</span>
        <span style="font-weight: bold;">self</span>.who += <span style="font-weight: bold;">self</span>.lr * numpy.dot((output_errors * final_outputs * (1.0 - final_outputs)), numpy.transpose(hidden_outputs))
        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">update the weights for the links between the hidden and output layers</span>
        <span style="font-weight: bold;">self</span>.wih += <span style="font-weight: bold;">self</span>.lr * numpy.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), numpy.transpose(inputs))
        <span style="font-weight: bold;">pass</span>

    <span style="font-weight: bold; font-style: italic;">#</span><span style="font-weight: bold; font-style: italic;">query the neural network</span>
    <span style="font-weight: bold;">def</span> <span style="font-weight: bold;">query</span>(<span style="font-weight: bold;">self</span>, inputs_list):
        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">convert inputs list to 2d array</span>
        <span style="font-weight: bold; font-style: italic;">inputs</span> = numpy.array(inputs_list, ndmin=2).T
        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">calculate signals into hidden layer</span>
        <span style="font-weight: bold; font-style: italic;">hidden_inputs</span> = numpy.dot(<span style="font-weight: bold;">self</span>.wih, inputs)
        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">calculate the signals emerging form hidden layer</span>
        <span style="font-weight: bold; font-style: italic;">hidden_outputs</span> = <span style="font-weight: bold;">self</span>.activation_function(hidden_inputs)
        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">calculate the signals into final output layer</span>
        <span style="font-weight: bold; font-style: italic;">final_inputs</span> = numpy.dot(<span style="font-weight: bold;">self</span>.who, hidden_outputs)
        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">calculate the signals emerging from final output layer</span>
        <span style="font-weight: bold; font-style: italic;">final_outputs</span> = <span style="font-weight: bold;">self</span>.activation_function(final_inputs)
        <span style="font-weight: bold;">return</span> final_outputs

</pre>
</div>

<p>
这些代码可以用于创建,训练和查询3层神经网络,进行几乎任何任务.下一步就可以进行
特定任务,学习识别手写数字.
</p>
</div>
</div>
</div>

<div id="outline-container-org1347e23" class="outline-3">
<h3 id="org1347e23">手写数字的数据集MNIST</h3>
<div class="outline-text-3" id="text-org1347e23">
<p>
手写数字的MNIST 数据集可以在网站 <a href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a> 里获得.
这个网页也列出了在学习和正确识别手写字符方面,新旧想法的表现如何.
这个MNIST数据库的格式不容易使用,因此其他人已经创建了相对简单的数据文件格式,参
见: <a href="https://pjreddie.com/projects/mnist-in-csv/">https://pjreddie.com/projects/mnist-in-csv/</a>
这些文件称为 CSV 文件,文件中每个值都有逗号分割.大部分电子表格和数据分析软件兼
容CSV文件.这个网站提供了两个CSV文件:
</p>
<ul class="org-ul">
<li>训练集: <a href="http://www.pjreddie.com/media/files/mnist_train.csv">http://www.pjreddie.com/media/files/mnist_train.csv</a></li>
<li>测试集: <a href="http://www.pjreddie.com/media/files/mnist_test.csv">http://www.pjreddie.com/media/files/mnist_test.csv</a></li>
</ul>

<p>
训练集是用来训练神经网络的60000个标记样本集.标记是指输入与期望的输出匹配,也就
是答案应该是多少.
训练集和测试集分开是为了确保使用神经网络之前没有见过的数据再次测试.
测试集数据太多,不利于我们进行代码的调试,所以我们可以先使用数据集的较小的子集
进行代码的调试.下面是 MNIST 中10条和100条记录的链接:
</p>
<ul class="org-ul">
<li>10条: <a href="https://raw.githubusercontent.com/makeyourownneuralnetwork/makeyourownneuralnetwork/master/mnist_dataset/mnist_test_10.csv">https://raw.githubusercontent.com/makeyourownneuralnetwork/makeyourownneuralnetwork/master/mnist_dataset/mnist_test_10.csv</a></li>
<li>100条: <a href="https://raw.githubusercontent.com/makeyourownneuralnetwork/makeyourownneuralnetwork/master/mnist_dataset/mnist_train_100.csv">https://raw.githubusercontent.com/makeyourownneuralnetwork/makeyourownneuralnetwork/master/mnist_dataset/mnist_train_100.csv</a></li>
</ul>

<p>
下面先使用 Python 来打开文件并获取其中数据:
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold; font-style: italic;">data_file</span> = <span style="font-weight: bold;">open</span>(<span style="font-style: italic;">"mnist_dataset/mnist_train_100.csv"</span>, <span style="font-style: italic;">'r'</span>)
<span style="font-weight: bold; font-style: italic;">data_list</span> = data_file.readlines()
data_file.close()
</pre>
</div>
<p>
第一行代码使用 open 函数打开一个文件,第一个参数是文件路径加上文件名,第二个参
数是可选的,告诉 Python 以什么方式处理文件, 'r' 表示用只读的方式打开文件.
open 函数返回一个文件句柄,或者说是一个指向文件的引用.这个句柄被分配给
data_file 变量.
第二行代码将文件中所有行读入变量 data_list. 这个变量包含一个列表,列表中的一项
表示文件中一行的字符串.
顺带一提, readlines() 会将整个文件读取到内存,有人会说不要使用这个方法,要一次
读取一行,对这行进行操作后,移动到下一行,这很好,不会出现电脑内存被沾满的情况,但
是我们的文件不是很大,使用 readlines() 会让代码更简单清晰.
最后一行代码关闭文件.打开文件使用完毕后关闭文件是一个很好的习惯,如果不这样做,
会造成问题,如果另一个文件同时打开了这个文件,并进行修改,这就像是两个人试图在同
一张纸上写字.
</p>

<p>
下面我们来试试这种方式打开10条记录的文件:
</p>
<div class="org-src-container">
<pre class="src src-jupyter-python">data_file = open("/home/kamisama/Downloads/mnist_test_10.csv", 'r')
data_list = data_file.readlines()
data_file.close()
len(data_list)
</pre>
</div>

<p>
接着我们看看读取的一行内容是什么:
</p>
<div class="org-src-container">
<pre class="src src-jupyter-python">data_list[0]
</pre>
</div>

<pre class="example">
7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,84,185,159,151,60,36,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,222,254,254,254,254,241,198,198,198,198,198,198,198,198,170,52,0,0,0,0,0,0,0,0,0,0,0,0,67,114,72,114,163,227,254,225,254,254,254,250,229,254,254,140,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,17,66,14,67,67,67,59,21,236,254,106,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,83,253,209,18,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,22,233,255,83,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,129,254,238,44,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,59,249,254,62,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,133,254,187,5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,9,205,248,58,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,126,254,182,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,75,251,240,57,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,19,221,254,166,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3,203,254,219,35,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,38,254,254,77,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,31,224,254,115,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,133,254,254,52,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,61,242,254,254,52,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,121,254,254,219,40,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,121,254,207,18,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n
</pre>

<p>
第一个数字是标签,也就是期望值,其余的数字是构成图像像素的颜色值,仔细观察可以发
现,这些值都在 0-255 之间.
</p>

<p>
先前我们使用 imshow() 函数绘制过矩形数组,类似的,这里我们有了构成图像的像素的
颜色值,只要将这些颜色值转换成矩阵形式,就可以绘制图像了.
步骤分解为下面几步:
</p>
<ul class="org-ul">
<li>将使用逗号分隔的文本字符串值拆分成单个值</li>
<li>忽略第一个值,因为它是标签,将剩下的 28 × 28 = 784 个值转换成 28列28行的数组</li>
<li>绘制数组</li>
</ul>

<div class="org-src-container">
<pre class="src src-jupyter-python">import numpy
import matplotlib.pyplot
%matplotlib inline
all_values = data_list[0].split(',')
image_array = numpy.asfarray(all_values[1:]).reshape((28,28))
matplotlib.pyplot.imshow(image_array, cmap='Greys', interpolation='None')
</pre>
</div>

<pre class="example">
&lt;matplotlib.image.AxesImage at 0x7f568b6c7eb0&gt;
</pre>


<div class="figure">
<p><img src="./ob-jupyter/783eb55c55f0ee77e2904757086b26102293c1a5.png" alt="783eb55c55f0ee77e2904757086b26102293c1a5.png" />
</p>
</div>
<p>
numpy.asfarray() 函数将文本字符串转换成实数,并创建这些数字的数组,reshape() 函
数确保数字列表每28个元素折返一次,形成28 × 28 的方形矩阵
</p>
</div>

<div id="outline-container-orgaef6384" class="outline-4">
<h4 id="orgaef6384">准备MNIST训练数据</h4>
<div class="outline-text-4" id="text-orgaef6384">
<p>
我们已经指导如何获取和拆开 MNIST 数据文件中的数据,并理解和可视化这些数据.我
们要使用这些数据训练神经网络之前,需要想想,我们应该如何准备数据.
</p>

<p>
先前的分析可以知道,如果输入数据和输出值正好在网络节点的激活函数的舒适区域内,
那神经网络会工作得更出色.
</p>

<p>
那我们要做的第一件事就是把输入的颜色值从较大的 0-255 范围,缩小到 0.01 到1.0
的范围,最低范围缩小到 0.01 是避免先前观察到输入值为0会认为导致权重更新失败,
输入值没有选择0.99,是因为输入为1.0并不会造成任何问题,只需要避免输出值为1.0.
</p>

<p>
将 0-255 的原始输入值除以 255,然后乘以0.99再加上0.01 就将整体输入值的范围变
成0.01-1.0.
下面是演示代码实现并打印调整后的值
</p>
<div class="org-src-container">
<pre class="src src-jupyter-python">scale_input = (numpy.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01
print(scale_input)
</pre>
</div>

<p>
下面再来考虑神经网络的输出,先前看到输出值应该匹配激活函数可以输出值的范围,我
们的罗奇函数不能输出像 -2.0或255 这样的数字,能输出的范围为 0.0 到1.0,实际上
并不能达到 0.0或1.0 因为这是逻辑函数的极限值,并不能真正达到.因此在训练时要调
整目标值.
</p>

<p>
实际上,我们应该考虑更深层次的问题,那就是输出应该是什么样子的? 是一个图片?这
就表示要输出 28 × 28 = 784 个输出节点.
</p>

<p>
我们要意识到,我们是为了要求神经网络对图像进行分类,匹配正确的标签.这些标签是
0-9共10个数字中的一个,这意味着,输出层应该有10个节点,每个节点都对应一个可能的
答案或标齐那.如果答案是0 输出层第一个节点激发,其余节点保持抑制状态.如果答案
是9 那最后一个输出层节点激发,其余节点保持抑制状态.下图阐释了这个方案:
<img src="imgs/00143.jpeg" alt="00143.jpeg" />
</p>

<p>
现在,我们将这种想法转化成目标数组.
如果训练样本的标签为5,那么除了标签5对应的节点,其他所有节点的值都应该很小,理
想情况可能是 [0,0,0,0,0,1,0,0,0,0,]
实际上,让神经网络输出0和1,对于激活函数来说是不可能的,这会导致大的权重和饱和
网络,因此要重新调整这些数字,我们用0.01 和 0.99来替代0和1,这样,标签为5的目标
输出数组应该是: [0.01, 0.01, 0.01, 0.01, 0.01, 0.99, 0.01, 0.01, 0.01, 0.01]
下面代码展示了创建目标矩阵:
</p>
<div class="org-src-container">
<pre class="src src-jupyter-python"># output nodes is 10 (example)
onodes = 10
targets = numpy.zeros(onodes) + 0.01
targets[int(all_values[0])] = 0.99
print(targets)
</pre>
</div>

<p>
现在已经准备用于训练的输入数据以及用于训练的输出数据,下面就可以更新Python 代
码,来实现这些操作了.
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python"># neural network class definition
import matplotlib.pyplot
import numpy
from scipy.special import expit
class neuralNetwork:
    #initialize the neutral network.
    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):
	# set number of nodes in each input,hidden,output layer
	self.inodes = inputnodes
	self.hnodes = hiddennodes
	self.onodes = outputnodes
	# learning rate
	self.lr = learningrate
	self.wih = numpy.random.normal(0.0, pow(self.hnodes, -0.5), (self.hnodes, self.inodes))
	self.who = numpy.random.normal(0.0, pow(self.onodes, -0.5), (self.onodes, self.hnodes))
	self.activation_function = lambda x: expit(x)
	pass
    # train the neural network
    def train(self, input_list, targets_list):
	# convert input list to 2d array
	inputs = numpy.array(input_list, ndmin=2).T
	targets = numpy.array(targets_list, ndmin=2).T
	# calculate signals into hidden layer
	hidden_inputs = numpy.dot(self.wih, inputs)
	# calculate signals emerging from hidden layer
	hidden_outputs = self.activation_function(hidden_inputs)
	# calculate signals into final output layer
	final_inputs = numpy.dot(self.who, hidden_inputs)
	# calculate signals emerging from final output layer
	final_outputs = self.activation_function(final_inputs)

	# error is the (target - actual)
	output_errors = targets - final_outputs

	# hidden layer error is the output_errors, split by weights, recombined at hidden nodes
	hidden_errors = numpy.dot(self.who.T, output_errors)

	# update the weights for the links between the hidden and output layers
	self.who += self.lr * numpy.dot((output_errors * final_outputs * (1.0 - final_outputs)), numpy.transpose(hidden_outputs))
	# update the weights for the links between the hidden and output layers
	self.wih += self.lr * numpy.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), numpy.transpose(inputs))
	pass

    #query the neural network
    def query(self, inputs_list):
	# convert inputs list to 2d array
	inputs = numpy.array(inputs_list, ndmin=2).T
	# calculate signals into hidden layer
	hidden_inputs = numpy.dot(self.wih, inputs)
	# calculate the signals emerging form hidden layer
	hidden_outputs = self.activation_function(hidden_inputs)
	# calculate the signals into final output layer
	final_inputs = numpy.dot(self.who, hidden_outputs)
	# calculate the signals emerging from final output layer
	final_outputs = self.activation_function(final_inputs)
	return final_outputs

# number of input hidden and output nodes
input_nodes = 784
hidden_nodes = 100
output_nodes = 10

# learning rate is 0.3
learning_rate = 0.3

# create instance of neural network
n = neuralNetwork(input_nodes, hidden_nodes, output_nodes, learning_rate)

# load the mnist training data CSV file into a list
training_data_file = open("/home/kamisama/Downloads/mnist_train_100.csv", 'r')
training_data_list = training_data_file.readlines()
training_data_file.close()

# train the neural network
# go through all records in the training data set
for record in training_data_list:
    # split the record by the ',' commas
    all_values = record.split(',')
    # scale and shift the inputs
    inputs = (numpy.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01
    # create the target output values (all 0.01, except the desired label which is 0.99)
    targets = numpy.zeros(output_nodes) + 0.01
    # all_values[0] is the target label for this record
    targets[int(all_values[0])] = 0.99
    n.train(inputs, targets)
</pre>
</div>
</div>
</div>

<div id="outline-container-org9c9dac6" class="outline-4">
<h4 id="org9c9dac6">测试网络</h4>
<div class="outline-text-4" id="text-org9c9dac6">
<p>
上面的代码已经使用了较小的子集数据(100条)来训练网络,我们来使用测试数据的10条
记录来测试神经网络
</p>

<p>
加载测试数据,并获取第一条数据标签:
</p>
<div class="org-src-container">
<pre class="src src-jupyter-python"># load the mnist test data CSV file in to a list
test_data_file = open("/home/kamisama/Downloads/mnist_test_10.csv", 'r')
test_data_list = test_data_file.readlines()
test_data_file.close()

#get the first test record
all_values = test_data_list[0].split(',')
print(all_values[0])
</pre>
</div>

<pre class="example">
7
</pre>


<p>
查看第一条测试数据的图像:
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python">image_array = numpy.asfarray(all_values[1:]).reshape((28,28))
matplotlib.pyplot.imshow(image_array, cmap="Greys", interpolation="None")
</pre>
</div>

<pre class="example">
&lt;matplotlib.image.AxesImage at 0x7fb305c00be0&gt;
</pre>


<div class="figure">
<p><img src="./ob-jupyter/05a79ae6b89fc614f93113e7fab9ecad95efcc70.png" alt="05a79ae6b89fc614f93113e7fab9ecad95efcc70.png" />
</p>
</div>

<p>
使用训练后的网络查询识别结果:
</p>
<div class="org-src-container">
<pre class="src src-jupyter-python">n.query((numpy.asfarray(all_values[1:])/ 255.0 * 0.99) + 0.01)
</pre>
</div>

<pre class="example">
array([[0.006904  ],
       [0.00218566],
       [0.09688954],
       [0.0140386 ],
       [0.0125424 ],
       [0.0067416 ],
       [0.00838635],
       [0.71919368],
       [0.00743467],
       [0.00100871]])
</pre>


<p>
我们使用100条训练记录训练了神经网络,并用训练后的网络识别了测试记录的第一条数
据,可以看到输出的数据矩阵中,第七个输出,也就是代表标签7的输出是所有输出里最大
的,那就表示我们这一次识别是成功的.
</p>

<p>
我们只用了100条记录训练神经网络,虽然测试结果有一定效果,但这就像我们掌握某项
技能一样,熟能生巧,练习100次的熟练度肯定没有练习10000次的熟练度高.我们可以开
始使用完整数据集训练,在此之前,我们先改写一下代码,使得代码可以对神经网络训练
成果的成功率进行记录,就像是打游戏一样记录游戏分数.
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python"># test the neural network
# scorecard for how well the network performs, initially empty.
scorecard = []
# go through all the records in the test data set
for record in test_data_list:
    # split the record by the '.' commas
    all_values = record.split(',')
    # correct answer is first value
    correct_label = int(all_values[0])
    print(correct_label, "correct label")
    # scale and shift the inputs
    inputs = (numpy.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01
    # query the network
    outputs = n.query(inputs)
    # the index of the highest value corresponds to the label
    label = numpy.argmax(outputs)
    print(label, "network's answer")
    # append correct or in correct to list
    if (label == correct_label):
	# network's answer matches correct answer, add 1 to scorecard
	scorecard.append(1)
    else:
	# network's answer doesn't match correct answer, add 0 to scorecard
	scorecard.append(0)
	pass
    pass

</pre>
</div>

<pre class="example">
7 correct label
7 network's answer
2 correct label
2 network's answer
1 correct label
7 network's answer
0 correct label
7 network's answer
4 correct label
7 network's answer
1 correct label
7 network's answer
4 correct label
7 network's answer
9 correct label
7 network's answer
5 correct label
7 network's answer
9 correct label
7 network's answer
</pre>

<p>
查看下计分卡的内容:
</p>
<div class="org-src-container">
<pre class="src src-jupyter-python">print(scorecard)
</pre>
</div>

<pre class="example">
[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]
</pre>


<p>
这次的结果就不理想了,得分很糟糕,但考虑到训练集很小,所以也不算太糟糕.失败了,
但没有完全失败.
</p>

<p>
看列表形式的计分板不是很好,数据多了就不好观察了,我们可以将结果转换成准确率:
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python"># calculate the performance score, the fraction of correct answers
scorecard_array = numpy.asarray(scorecard)
print("performance = ", scorecard_array.sum()/scorecard_array.size)
</pre>
</div>

<pre class="example">
performance =  0.1
</pre>


<p>
可以观察到训练成果并不是很理想,但是我们已经有了一个可以调整数据集,观察训练结
果的代码了,完整的代码如下:
</p>
<div class="org-src-container">
<pre class="src src-jupyter-python"># neural network class definition
import matplotlib.pyplot
import numpy
from scipy.special import expit
class neuralNetwork:
    #initialize the neutral network.
    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):
	# set number of nodes in each input,hidden,output layer
	self.inodes = inputnodes
	self.hnodes = hiddennodes
	self.onodes = outputnodes
	# learning rate
	self.lr = learningrate
	self.wih = numpy.random.normal(0.0, pow(self.hnodes, -0.5), (self.hnodes, self.inodes))
	self.who = numpy.random.normal(0.0, pow(self.onodes, -0.5), (self.onodes, self.hnodes))
	self.activation_function = lambda x: expit(x)
	pass
    # train the neural network
    def train(self, input_list, targets_list):
	# convert input list to 2d array
	inputs = numpy.array(input_list, ndmin=2).T
	targets = numpy.array(targets_list, ndmin=2).T
	# calculate signals into hidden layer
	hidden_inputs = numpy.dot(self.wih, inputs)
	# calculate signals emerging from hidden layer
	hidden_outputs = self.activation_function(hidden_inputs)
	# calculate signals into final output layer
	final_inputs = numpy.dot(self.who, hidden_inputs)
	# calculate signals emerging from final output layer
	final_outputs = self.activation_function(final_inputs)

	# error is the (target - actual)
	output_errors = targets - final_outputs

	# hidden layer error is the output_errors, split by weights, recombined at hidden nodes
	hidden_errors = numpy.dot(self.who.T, output_errors)

	# update the weights for the links between the hidden and output layers
	self.who += self.lr * numpy.dot((output_errors * final_outputs * (1.0 - final_outputs)), numpy.transpose(hidden_outputs))
	# update the weights for the links between the hidden and output layers
	self.wih += self.lr * numpy.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), numpy.transpose(inputs))
	pass

    #query the neural network
    def query(self, inputs_list):
	# convert inputs list to 2d array
	inputs = numpy.array(inputs_list, ndmin=2).T
	# calculate signals into hidden layer
	hidden_inputs = numpy.dot(self.wih, inputs)
	# calculate the signals emerging form hidden layer
	hidden_outputs = self.activation_function(hidden_inputs)
	# calculate the signals into final output layer
	final_inputs = numpy.dot(self.who, hidden_outputs)
	# calculate the signals emerging from final output layer
	final_outputs = self.activation_function(final_inputs)
	return final_outputs

# number of input hidden and output nodes
input_nodes = 784
hidden_nodes = 100
output_nodes = 10

# learning rate is 0.3
learning_rate = 0.3

# create instance of neural network
n = neuralNetwork(input_nodes, hidden_nodes, output_nodes, learning_rate)

# load the mnist training data CSV file into a list
training_data_file = open("/home/kamisama/Downloads/mnist_train_100.csv", 'r')
training_data_list = training_data_file.readlines()
training_data_file.close()

# train the neural network
# go through all records in the training data set
for record in training_data_list:
    # split the record by the ',' commas
    all_values = record.split(',')
    # scale and shift the inputs
    inputs = (numpy.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01
    # create the target output values (all 0.01, except the desired label which is 0.99)
    targets = numpy.zeros(output_nodes) + 0.01
    # all_values[0] is the target label for this record
    targets[int(all_values[0])] = 0.99
    n.train(inputs, targets)

# test the neural network
# scorecard for how well the network performs, initially empty.
scorecard = []
# go through all the records in the test data set
for record in test_data_list:
    # split the record by the '.' commas
    all_values = record.split(',')
    # correct answer is first value
    correct_label = int(all_values[0])
    print(correct_label, "correct label")
    # scale and shift the inputs
    inputs = (numpy.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01
    # query the network
    outputs = n.query(inputs)
    # the index of the highest value corresponds to the label
    label = numpy.argmax(outputs)
    print(label, "network's answer")
    # append correct or in correct to list
    if (label == correct_label):
	# network's answer matches correct answer, add 1 to scorecard
	scorecard.append(1)
    else:
	# network's answer doesn't match correct answer, add 0 to scorecard
	scorecard.append(0)
	pass
    pass

# calculate the performance score, the fraction of correct answers
scorecard_array = numpy.asarray(scorecard)
print("performance = ", scorecard_array.sum()/scorecard_array.size)
</pre>
</div>

<pre class="example">
7 correct label
7 network's answer
2 correct label
0 network's answer
1 correct label
4 network's answer
0 correct label
7 network's answer
4 correct label
4 network's answer
1 correct label
4 network's answer
4 correct label
4 network's answer
9 correct label
4 network's answer
5 correct label
4 network's answer
9 correct label
4 network's answer
performance =  0.3
</pre>
</div>
</div>

<div id="outline-container-org02f5a48" class="outline-4">
<h4 id="org02f5a48">使用完整的数据集进行训练和测试</h4>
<div class="outline-text-4" id="text-org02f5a48">
<p>
上一节最后的代码已经包含了训练和测试网络性能的代码了,此时改变文件名,就可以使
用完整的训练数据集和测试数据集进行训练了.
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python"># neural network class definition
import matplotlib.pyplot
import numpy
from scipy.special import expit
class neuralNetwork:
    #initialize the neutral network.
    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):
	# set number of nodes in each input,hidden,output layer
	self.inodes = inputnodes
	self.hnodes = hiddennodes
	self.onodes = outputnodes
	# learning rate
	self.lr = learningrate
	self.wih = numpy.random.normal(0.0, pow(self.hnodes, -0.5), (self.hnodes, self.inodes))
	self.who = numpy.random.normal(0.0, pow(self.onodes, -0.5), (self.onodes, self.hnodes))
	self.activation_function = lambda x: expit(x)
	pass
    # train the neural network
    def train(self, input_list, targets_list):
	# convert input list to 2d array
	inputs = numpy.array(input_list, ndmin=2).T
	targets = numpy.array(targets_list, ndmin=2).T
	# calculate signals into hidden layer
	hidden_inputs = numpy.dot(self.wih, inputs)
	# calculate signals emerging from hidden layer
	hidden_outputs = self.activation_function(hidden_inputs)
	# calculate signals into final output layer
	final_inputs = numpy.dot(self.who, hidden_outputs)
	# calculate signals emerging from final output layer
	final_outputs = self.activation_function(final_inputs)

	# error is the (target - actual)
	output_errors = targets - final_outputs

	# hidden layer error is the output_errors, split by weights, recombined at hidden nodes
	hidden_errors = numpy.dot(self.who.T, output_errors)

	# update the weights for the links between the hidden and output layers
	self.who += self.lr * numpy.dot((output_errors * final_outputs * (1.0 - final_outputs)), numpy.transpose(hidden_outputs))
	# update the weights for the links between the hidden and output layers
	self.wih += self.lr * numpy.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), numpy.transpose(inputs))
	pass

    #query the neural network
    def query(self, inputs_list):
	# convert inputs list to 2d array
	inputs = numpy.array(inputs_list, ndmin=2).T
	# calculate signals into hidden layer
	hidden_inputs = numpy.dot(self.wih, inputs)
	# calculate the signals emerging form hidden layer
	hidden_outputs = self.activation_function(hidden_inputs)
	# calculate the signals into final output layer
	final_inputs = numpy.dot(self.who, hidden_outputs)
	# calculate the signals emerging from final output layer
	final_outputs = self.activation_function(final_inputs)
	return final_outputs

# number of input hidden and output nodes
input_nodes = 784
hidden_nodes = 100
output_nodes = 10

# learning rate is 0.3
learning_rate = 0.3

# create instance of neural network
n = neuralNetwork(input_nodes, hidden_nodes, output_nodes, learning_rate)

# load the mnist training data CSV file into a list
training_data_file = open("/home/kamisama/Downloads/mnist_train.csv", 'r')
training_data_list = training_data_file.readlines()
training_data_file.close()

# train the neural network
# go through all records in the training data set
for record in training_data_list:
    # split the record by the ',' commas
    all_values = record.split(',')
    # scale and shift the inputs
    inputs = (numpy.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01
    # create the target output values (all 0.01, except the desired label which is 0.99)
    targets = numpy.zeros(output_nodes) + 0.01
    # all_values[0] is the target label for this record
    targets[int(all_values[0])] = 0.99
    n.train(inputs, targets)

# test the neural network
test_data_file = open("/home/kamisama/Downloads/mnist_test.csv", 'r')
test_data_list = test_data_file.readlines()
test_data_file.close()
# scorecard for how well the network performs, initially empty.
scorecard = []
# go through all the records in the test data set
for record in test_data_list:
    # split the record by the '.' commas
    all_values = record.split(',')
    # correct answer is first value
    correct_label = int(all_values[0])
    # print(correct_label, "correct label")
    # scale and shift the inputs
    inputs = (numpy.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01
    # query the network
    outputs = n.query(inputs)
    # the index of the highest value corresponds to the label
    label = numpy.argmax(outputs)
    # print(label, "network's answer")
    # append correct or in correct to list
    if (label == correct_label):
	# network's answer matches correct answer, add 1 to scorecard
	scorecard.append(1)
    else:
	# network's answer doesn't match correct answer, add 0 to scorecard
	scorecard.append(0)
	pass
    pass

# calculate the performance score, the fraction of correct answers
scorecard_array = numpy.asarray(scorecard)
print("performance = ", scorecard_array.sum()/scorecard_array.size)
</pre>
</div>

<pre class="example">
performance =  0.9495
</pre>
</div>
</div>

<div id="outline-container-org7b1cb44" class="outline-4">
<h4 id="org7b1cb44">一些改进:调整学习率</h4>
<div class="outline-text-4" id="text-org7b1cb44">
<p>
完成的第一个神经网络,只用了简单的思路,简单的代码,就可以在 MNIST 数据集上获得
准确率大致为95%的性能分数,那我们看看是否可以进行一些简单的改进.
第一个尝试的改进是调整学习率,第一部分里学习率的作用是让神经网络进行适度调整,
决定梯度下降时下降过程的幅度.
</p>

<p>
我们上面直接使用了学习率为0.3进行训练,下面试试学习率设置为0.6,看看性能如何:
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python"># neural network class definition
import matplotlib.pyplot
import numpy
from scipy.special import expit
class neuralNetwork:
    #initialize the neutral network.
    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):
	# set number of nodes in each input,hidden,output layer
	self.inodes = inputnodes
	self.hnodes = hiddennodes
	self.onodes = outputnodes
	# learning rate
	self.lr = learningrate
	self.wih = numpy.random.normal(0.0, pow(self.hnodes, -0.5), (self.hnodes, self.inodes))
	self.who = numpy.random.normal(0.0, pow(self.onodes, -0.5), (self.onodes, self.hnodes))
	self.activation_function = lambda x: expit(x)
	pass
    # train the neural network
    def train(self, input_list, targets_list):
	# convert input list to 2d array
	inputs = numpy.array(input_list, ndmin=2).T
	targets = numpy.array(targets_list, ndmin=2).T
	# calculate signals into hidden layer
	hidden_inputs = numpy.dot(self.wih, inputs)
	# calculate signals emerging from hidden layer
	hidden_outputs = self.activation_function(hidden_inputs)
	# calculate signals into final output layer
	final_inputs = numpy.dot(self.who, hidden_outputs)
	# calculate signals emerging from final output layer
	final_outputs = self.activation_function(final_inputs)

	# error is the (target - actual)
	output_errors = targets - final_outputs

	# hidden layer error is the output_errors, split by weights, recombined at hidden nodes
	hidden_errors = numpy.dot(self.who.T, output_errors)

	# update the weights for the links between the hidden and output layers
	self.who += self.lr * numpy.dot((output_errors * final_outputs * (1.0 - final_outputs)), numpy.transpose(hidden_outputs))
	# update the weights for the links between the hidden and output layers
	self.wih += self.lr * numpy.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), numpy.transpose(inputs))
	pass

    #query the neural network
    def query(self, inputs_list):
	# convert inputs list to 2d array
	inputs = numpy.array(inputs_list, ndmin=2).T
	# calculate signals into hidden layer
	hidden_inputs = numpy.dot(self.wih, inputs)
	# calculate the signals emerging form hidden layer
	hidden_outputs = self.activation_function(hidden_inputs)
	# calculate the signals into final output layer
	final_inputs = numpy.dot(self.who, hidden_outputs)
	# calculate the signals emerging from final output layer
	final_outputs = self.activation_function(final_inputs)
	return final_outputs

# number of input hidden and output nodes
input_nodes = 784
hidden_nodes = 100
output_nodes = 10

# learning rate is 0.3
learning_rate = 0.6

# create instance of neural network
n = neuralNetwork(input_nodes, hidden_nodes, output_nodes, learning_rate)

# load the mnist training data CSV file into a list
training_data_file = open("/home/kamisama/Downloads/mnist_train.csv", 'r')
training_data_list = training_data_file.readlines()
training_data_file.close()

# train the neural network
# go through all records in the training data set
for record in training_data_list:
    # split the record by the ',' commas
    all_values = record.split(',')
    # scale and shift the inputs
    inputs = (numpy.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01
    # create the target output values (all 0.01, except the desired label which is 0.99)
    targets = numpy.zeros(output_nodes) + 0.01
    # all_values[0] is the target label for this record
    targets[int(all_values[0])] = 0.99
    n.train(inputs, targets)

# test the neural network
test_data_file = open("/home/kamisama/Downloads/mnist_test.csv", 'r')
test_data_list = test_data_file.readlines()
test_data_file.close()
# scorecard for how well the network performs, initially empty.
scorecard = []
# go through all the records in the test data set
for record in test_data_list:
    # split the record by the '.' commas
    all_values = record.split(',')
    # correct answer is first value
    correct_label = int(all_values[0])
    # print(correct_label, "correct label")
    # scale and shift the inputs
    inputs = (numpy.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01
    # query the network
    outputs = n.query(inputs)
    # the index of the highest value corresponds to the label
    label = numpy.argmax(outputs)
    # print(label, "network's answer")
    # append correct or in correct to list
    if (label == correct_label):
	# network's answer matches correct answer, add 1 to scorecard
	scorecard.append(1)
    else:
	# network's answer doesn't match correct answer, add 0 to scorecard
	scorecard.append(0)
	pass
    pass

# calculate the performance score, the fraction of correct answers
scorecard_array = numpy.asarray(scorecard)
print("performance = ", scorecard_array.sum()/scorecard_array.size)
</pre>
</div>

<pre class="example">
performance =  0.8944
</pre>


<p>
再使用0.1的学习率试一次:
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python"># neural network class definition
import matplotlib.pyplot
import numpy
from scipy.special import expit
class neuralNetwork:
    #initialize the neutral network.
    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):
	# set number of nodes in each input,hidden,output layer
	self.inodes = inputnodes
	self.hnodes = hiddennodes
	self.onodes = outputnodes
	# learning rate
	self.lr = learningrate
	self.wih = numpy.random.normal(0.0, pow(self.hnodes, -0.5), (self.hnodes, self.inodes))
	self.who = numpy.random.normal(0.0, pow(self.onodes, -0.5), (self.onodes, self.hnodes))
	self.activation_function = lambda x: expit(x)
	pass
    # train the neural network
    def train(self, input_list, targets_list):
	# convert input list to 2d array
	inputs = numpy.array(input_list, ndmin=2).T
	targets = numpy.array(targets_list, ndmin=2).T
	# calculate signals into hidden layer
	hidden_inputs = numpy.dot(self.wih, inputs)
	# calculate signals emerging from hidden layer
	hidden_outputs = self.activation_function(hidden_inputs)
	# calculate signals into final output layer
	final_inputs = numpy.dot(self.who, hidden_outputs)
	# calculate signals emerging from final output layer
	final_outputs = self.activation_function(final_inputs)

	# error is the (target - actual)
	output_errors = targets - final_outputs

	# hidden layer error is the output_errors, split by weights, recombined at hidden nodes
	hidden_errors = numpy.dot(self.who.T, output_errors)

	# update the weights for the links between the hidden and output layers
	self.who += self.lr * numpy.dot((output_errors * final_outputs * (1.0 - final_outputs)), numpy.transpose(hidden_outputs))
	# update the weights for the links between the hidden and output layers
	self.wih += self.lr * numpy.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), numpy.transpose(inputs))
	pass

    #query the neural network
    def query(self, inputs_list):
	# convert inputs list to 2d array
	inputs = numpy.array(inputs_list, ndmin=2).T
	# calculate signals into hidden layer
	hidden_inputs = numpy.dot(self.wih, inputs)
	# calculate the signals emerging form hidden layer
	hidden_outputs = self.activation_function(hidden_inputs)
	# calculate the signals into final output layer
	final_inputs = numpy.dot(self.who, hidden_outputs)
	# calculate the signals emerging from final output layer
	final_outputs = self.activation_function(final_inputs)
	return final_outputs

# number of input hidden and output nodes
input_nodes = 784
hidden_nodes = 100
output_nodes = 10

# learning rate is 0.3
learning_rate = 0.1

# create instance of neural network
n = neuralNetwork(input_nodes, hidden_nodes, output_nodes, learning_rate)

# load the mnist training data CSV file into a list
training_data_file = open("/home/kamisama/Downloads/mnist_train.csv", 'r')
training_data_list = training_data_file.readlines()
training_data_file.close()

# train the neural network
# go through all records in the training data set
for record in training_data_list:
    # split the record by the ',' commas
    all_values = record.split(',')
    # scale and shift the inputs
    inputs = (numpy.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01
    # create the target output values (all 0.01, except the desired label which is 0.99)
    targets = numpy.zeros(output_nodes) + 0.01
    # all_values[0] is the target label for this record
    targets[int(all_values[0])] = 0.99
    n.train(inputs, targets)

# test the neural network
test_data_file = open("/home/kamisama/Downloads/mnist_test.csv", 'r')
test_data_list = test_data_file.readlines()
test_data_file.close()
# scorecard for how well the network performs, initially empty.
scorecard = []
# go through all the records in the test data set
for record in test_data_list:
    # split the record by the '.' commas
    all_values = record.split(',')
    # correct answer is first value
    correct_label = int(all_values[0])
    # print(correct_label, "correct label")
    # scale and shift the inputs
    inputs = (numpy.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01
    # query the network
    outputs = n.query(inputs)
    # the index of the highest value corresponds to the label
    label = numpy.argmax(outputs)
    # print(label, "network's answer")
    # append correct or in correct to list
    if (label == correct_label):
	# network's answer matches correct answer, add 1 to scorecard
	scorecard.append(1)
    else:
	# network's answer doesn't match correct answer, add 0 to scorecard
	scorecard.append(0)
	pass
    pass

# calculate the performance score, the fraction of correct answers
scorecard_array = numpy.asarray(scorecard)
print("performance = ", scorecard_array.sum()/scorecard_array.size)
</pre>
</div>

<pre class="example">
performance =  0.9537
</pre>


<p>
用0.2呢? :
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python"># neural network class definition
import matplotlib.pyplot
import numpy
from scipy.special import expit
class neuralNetwork:
    #initialize the neutral network.
    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):
	# set number of nodes in each input,hidden,output layer
	self.inodes = inputnodes
	self.hnodes = hiddennodes
	self.onodes = outputnodes
	# learning rate
	self.lr = learningrate
	self.wih = numpy.random.normal(0.0, pow(self.hnodes, -0.5), (self.hnodes, self.inodes))
	self.who = numpy.random.normal(0.0, pow(self.onodes, -0.5), (self.onodes, self.hnodes))
	self.activation_function = lambda x: expit(x)
	pass
    # train the neural network
    def train(self, input_list, targets_list):
	# convert input list to 2d array
	inputs = numpy.array(input_list, ndmin=2).T
	targets = numpy.array(targets_list, ndmin=2).T
	# calculate signals into hidden layer
	hidden_inputs = numpy.dot(self.wih, inputs)
	# calculate signals emerging from hidden layer
	hidden_outputs = self.activation_function(hidden_inputs)
	# calculate signals into final output layer
	final_inputs = numpy.dot(self.who, hidden_outputs)
	# calculate signals emerging from final output layer
	final_outputs = self.activation_function(final_inputs)

	# error is the (target - actual)
	output_errors = targets - final_outputs

	# hidden layer error is the output_errors, split by weights, recombined at hidden nodes
	hidden_errors = numpy.dot(self.who.T, output_errors)

	# update the weights for the links between the hidden and output layers
	self.who += self.lr * numpy.dot((output_errors * final_outputs * (1.0 - final_outputs)), numpy.transpose(hidden_outputs))
	# update the weights for the links between the hidden and output layers
	self.wih += self.lr * numpy.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), numpy.transpose(inputs))
	pass

    #query the neural network
    def query(self, inputs_list):
	# convert inputs list to 2d array
	inputs = numpy.array(inputs_list, ndmin=2).T
	# calculate signals into hidden layer
	hidden_inputs = numpy.dot(self.wih, inputs)
	# calculate the signals emerging form hidden layer
	hidden_outputs = self.activation_function(hidden_inputs)
	# calculate the signals into final output layer
	final_inputs = numpy.dot(self.who, hidden_outputs)
	# calculate the signals emerging from final output layer
	final_outputs = self.activation_function(final_inputs)
	return final_outputs

# number of input hidden and output nodes
input_nodes = 784
hidden_nodes = 100
output_nodes = 10

# learning rate is 0.3
learning_rate = 0.2

# create instance of neural network
n = neuralNetwork(input_nodes, hidden_nodes, output_nodes, learning_rate)

# load the mnist training data CSV file into a list
training_data_file = open("/home/kamisama/Downloads/mnist_train.csv", 'r')
training_data_list = training_data_file.readlines()
training_data_file.close()

# train the neural network
# go through all records in the training data set
for record in training_data_list:
    # split the record by the ',' commas
    all_values = record.split(',')
    # scale and shift the inputs
    inputs = (numpy.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01
    # create the target output values (all 0.01, except the desired label which is 0.99)
    targets = numpy.zeros(output_nodes) + 0.01
    # all_values[0] is the target label for this record
    targets[int(all_values[0])] = 0.99
    n.train(inputs, targets)

# test the neural network
test_data_file = open("/home/kamisama/Downloads/mnist_test.csv", 'r')
test_data_list = test_data_file.readlines()
test_data_file.close()
# scorecard for how well the network performs, initially empty.
scorecard = []
# go through all the records in the test data set
for record in test_data_list:
    # split the record by the '.' commas
    all_values = record.split(',')
    # correct answer is first value
    correct_label = int(all_values[0])
    # print(correct_label, "correct label")
    # scale and shift the inputs
    inputs = (numpy.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01
    # query the network
    outputs = n.query(inputs)
    # the index of the highest value corresponds to the label
    label = numpy.argmax(outputs)
    # print(label, "network's answer")
    # append correct or in correct to list
    if (label == correct_label):
	# network's answer matches correct answer, add 1 to scorecard
	scorecard.append(1)
    else:
	# network's answer doesn't match correct answer, add 0 to scorecard
	scorecard.append(0)
	pass
    pass

# calculate the performance score, the fraction of correct answers
scorecard_array = numpy.asarray(scorecard)
print("performance = ", scorecard_array.sum()/scorecard_array.size)
</pre>
</div>

<pre class="example">
performance =  0.9561
</pre>


<p>
使用更小的0.01呢?
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python"># neural network class definition
import matplotlib.pyplot
import numpy
from scipy.special import expit
class neuralNetwork:
    #initialize the neutral network.
    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):
	# set number of nodes in each input,hidden,output layer
	self.inodes = inputnodes
	self.hnodes = hiddennodes
	self.onodes = outputnodes
	# learning rate
	self.lr = learningrate
	self.wih = numpy.random.normal(0.0, pow(self.hnodes, -0.5), (self.hnodes, self.inodes))
	self.who = numpy.random.normal(0.0, pow(self.onodes, -0.5), (self.onodes, self.hnodes))
	self.activation_function = lambda x: expit(x)
	pass
    # train the neural network
    def train(self, input_list, targets_list):
	# convert input list to 2d array
	inputs = numpy.array(input_list, ndmin=2).T
	targets = numpy.array(targets_list, ndmin=2).T
	# calculate signals into hidden layer
	hidden_inputs = numpy.dot(self.wih, inputs)
	# calculate signals emerging from hidden layer
	hidden_outputs = self.activation_function(hidden_inputs)
	# calculate signals into final output layer
	final_inputs = numpy.dot(self.who, hidden_outputs)
	# calculate signals emerging from final output layer
	final_outputs = self.activation_function(final_inputs)

	# error is the (target - actual)
	output_errors = targets - final_outputs

	# hidden layer error is the output_errors, split by weights, recombined at hidden nodes
	hidden_errors = numpy.dot(self.who.T, output_errors)

	# update the weights for the links between the hidden and output layers
	self.who += self.lr * numpy.dot((output_errors * final_outputs * (1.0 - final_outputs)), numpy.transpose(hidden_outputs))
	# update the weights for the links between the hidden and output layers
	self.wih += self.lr * numpy.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), numpy.transpose(inputs))
	pass

    #query the neural network
    def query(self, inputs_list):
	# convert inputs list to 2d array
	inputs = numpy.array(inputs_list, ndmin=2).T
	# calculate signals into hidden layer
	hidden_inputs = numpy.dot(self.wih, inputs)
	# calculate the signals emerging form hidden layer
	hidden_outputs = self.activation_function(hidden_inputs)
	# calculate the signals into final output layer
	final_inputs = numpy.dot(self.who, hidden_outputs)
	# calculate the signals emerging from final output layer
	final_outputs = self.activation_function(final_inputs)
	return final_outputs

# number of input hidden and output nodes
input_nodes = 784
hidden_nodes = 100
output_nodes = 10

# learning rate is 0.3
learning_rate = 0.01

# create instance of neural network
n = neuralNetwork(input_nodes, hidden_nodes, output_nodes, learning_rate)

# load the mnist training data CSV file into a list
training_data_file = open("/home/kamisama/Downloads/mnist_train.csv", 'r')
training_data_list = training_data_file.readlines()
training_data_file.close()

# train the neural network
# go through all records in the training data set
for record in training_data_list:
    # split the record by the ',' commas
    all_values = record.split(',')
    # scale and shift the inputs
    inputs = (numpy.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01
    # create the target output values (all 0.01, except the desired label which is 0.99)
    targets = numpy.zeros(output_nodes) + 0.01
    # all_values[0] is the target label for this record
    targets[int(all_values[0])] = 0.99
    n.train(inputs, targets)

# test the neural network
test_data_file = open("/home/kamisama/Downloads/mnist_test.csv", 'r')
test_data_list = test_data_file.readlines()
test_data_file.close()
# scorecard for how well the network performs, initially empty.
scorecard = []
# go through all the records in the test data set
for record in test_data_list:
    # split the record by the '.' commas
    all_values = record.split(',')
    # correct answer is first value
    correct_label = int(all_values[0])
    # print(correct_label, "correct label")
    # scale and shift the inputs
    inputs = (numpy.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01
    # query the network
    outputs = n.query(inputs)
    # the index of the highest value corresponds to the label
    label = numpy.argmax(outputs)
    # print(label, "network's answer")
    # append correct or in correct to list
    if (label == correct_label):
	# network's answer matches correct answer, add 1 to scorecard
	scorecard.append(1)
    else:
	# network's answer doesn't match correct answer, add 0 to scorecard
	scorecard.append(0)
	pass
    pass

# calculate the performance score, the fraction of correct answers
scorecard_array = numpy.asarray(scorecard)
print("performance = ", scorecard_array.sum()/scorecard_array.size)
</pre>
</div>

<pre class="example">
performance =  0.9238
</pre>


<p>
我们会发现过大或过小的学习率都会导致性能下降,用梯度下降的图解释就是下降的步
长过大或过小都会影响学习效果.
</p>

<p>
实验过程会发现学习率是存在一个甜蜜点的,在这个点附近的学习率能在当前数据集和
神经网络中取得较好的训练效果.
</p>
</div>
</div>

<div id="outline-container-orgac09740" class="outline-4">
<h4 id="orgac09740">一些改进:多次运行</h4>
<div class="outline-text-4" id="text-orgac09740">
<p>
接下来的改进是可以使用数据集进行重复多次的训练.
有些人把训练一次称为一个世代,因此,10个世代的训练,意味着整个训练数据集运行程
序10次.这样的做法就是让梯度下降时有更多的爬下斜坡的机会.
试一下使用2个世代,只需要在训练代码外围添加额外的循环即可.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">train the neural network</span>
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">go through all records in the training data set</span>
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">epochs is the number of times the training data set is used for training.</span>
<span style="font-weight: bold; font-style: italic;">epochs</span> = 2
<span style="font-weight: bold;">for</span> e <span style="font-weight: bold;">in</span> <span style="font-weight: bold;">range</span>(epochs):
    <span style="font-weight: bold;">for</span> record <span style="font-weight: bold;">in</span> training_data_list:
        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">split the record by the ',' commas</span>
        <span style="font-weight: bold; font-style: italic;">all_values</span> = record.split(<span style="font-style: italic;">','</span>)
        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">scale and shift the inputs</span>
        <span style="font-weight: bold; font-style: italic;">inputs</span> = (numpy.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01
        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">create the target output values (all 0.01, except the desired label which is 0.99)</span>
        <span style="font-weight: bold; font-style: italic;">targets</span> = numpy.zeros(output_nodes) + 0.01
        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">all_values[0] is the target label for this record</span>
        targets[<span style="font-weight: bold;">int</span>(all_values[0])] = 0.99
        n.train(inputs, targets)
</pre>
</div>

<p>
修改后的网络得到的性能:
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python"># neural network class definition
import matplotlib.pyplot
import numpy
from scipy.special import expit
class neuralNetwork:
    #initialize the neutral network.
    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):
	# set number of nodes in each input,hidden,output layer
	self.inodes = inputnodes
	self.hnodes = hiddennodes
	self.onodes = outputnodes
	# learning rate
	self.lr = learningrate
	self.wih = numpy.random.normal(0.0, pow(self.hnodes, -0.5), (self.hnodes, self.inodes))
	self.who = numpy.random.normal(0.0, pow(self.onodes, -0.5), (self.onodes, self.hnodes))
	self.activation_function = lambda x: expit(x)
	pass
    # train the neural network
    def train(self, input_list, targets_list):
	# convert input list to 2d array
	inputs = numpy.array(input_list, ndmin=2).T
	targets = numpy.array(targets_list, ndmin=2).T
	# calculate signals into hidden layer
	hidden_inputs = numpy.dot(self.wih, inputs)
	# calculate signals emerging from hidden layer
	hidden_outputs = self.activation_function(hidden_inputs)
	# calculate signals into final output layer
	final_inputs = numpy.dot(self.who, hidden_outputs)
	# calculate signals emerging from final output layer
	final_outputs = self.activation_function(final_inputs)

	# error is the (target - actual)
	output_errors = targets - final_outputs

	# hidden layer error is the output_errors, split by weights, recombined at hidden nodes
	hidden_errors = numpy.dot(self.who.T, output_errors)

	# update the weights for the links between the hidden and output layers
	self.who += self.lr * numpy.dot((output_errors * final_outputs * (1.0 - final_outputs)), numpy.transpose(hidden_outputs))
	# update the weights for the links between the hidden and output layers
	self.wih += self.lr * numpy.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), numpy.transpose(inputs))
	pass

    #query the neural network
    def query(self, inputs_list):
	# convert inputs list to 2d array
	inputs = numpy.array(inputs_list, ndmin=2).T
	# calculate signals into hidden layer
	hidden_inputs = numpy.dot(self.wih, inputs)
	# calculate the signals emerging form hidden layer
	hidden_outputs = self.activation_function(hidden_inputs)
	# calculate the signals into final output layer
	final_inputs = numpy.dot(self.who, hidden_outputs)
	# calculate the signals emerging from final output layer
	final_outputs = self.activation_function(final_inputs)
	return final_outputs

# number of input hidden and output nodes
input_nodes = 784
hidden_nodes = 100
output_nodes = 10

# learning rate is 0.3
learning_rate = 0.3

# create instance of neural network
n = neuralNetwork(input_nodes, hidden_nodes, output_nodes, learning_rate)

# load the mnist training data CSV file into a list
training_data_file = open("/home/kamisama/Downloads/mnist_train.csv", 'r')
training_data_list = training_data_file.readlines()
training_data_file.close()

# train the neural network
# go through all records in the training data set
# epochs is the number of times the training data set is used for training.
epochs = 2
for e in range(epochs):
    for record in training_data_list:
	# split the record by the ',' commas
	all_values = record.split(',')
	# scale and shift the inputs
	inputs = (numpy.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01
	# create the target output values (all 0.01, except the desired label which is 0.99)
	targets = numpy.zeros(output_nodes) + 0.01
	# all_values[0] is the target label for this record
	targets[int(all_values[0])] = 0.99
	n.train(inputs, targets)

# test the neural network
test_data_file = open("/home/kamisama/Downloads/mnist_test.csv", 'r')
test_data_list = test_data_file.readlines()
test_data_file.close()
# scorecard for how well the network performs, initially empty.
scorecard = []
# go through all the records in the test data set
for record in test_data_list:
    # split the record by the '.' commas
    all_values = record.split(',')
    # correct answer is first value
    correct_label = int(all_values[0])
    # print(correct_label, "correct label")
    # scale and shift the inputs
    inputs = (numpy.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01
    # query the network
    outputs = n.query(inputs)
    # the index of the highest value corresponds to the label
    label = numpy.argmax(outputs)
    # print(label, "network's answer")
    # append correct or in correct to list
    if (label == correct_label):
	# network's answer matches correct answer, add 1 to scorecard
	scorecard.append(1)
    else:
	# network's answer doesn't match correct answer, add 0 to scorecard
	scorecard.append(0)
	pass
    pass

# calculate the performance score, the fraction of correct answers
scorecard_array = numpy.asarray(scorecard)
print("performance = ", scorecard_array.sum()/scorecard_array.size)
</pre>
</div>

<pre class="example">
performance =  0.9546
</pre>


<p>
在学习率为0.3使用1个世代的神经网络里,得到的性能是 <a href="#org02f5a48">使用完整的数据集进行训练和
测试</a> 只提高了一点点.
像调整学习率一样,使用几个不同的世代进行实验,直觉告诉我们,训练越多,性能越好,
但实验告诉我们,太多训练会过犹不及,因为网络过度拟合训练数据,因此网络在没有见
过的新数据上表现不佳.
下面是世代和性能的实验情况表格:
</p>

<p>
<img src="imgs/00150.jpeg" alt="00150.jpeg" />
从上面的表格里看到,大约在5或7个世代时,有一个甜蜜点,在这之后,性能会下降,这可
能是过度拟合的效果,性能在6个世代下降,可能是运行中出了问题,导致网络在梯度下降
过程中被卡在了一个局部的最小值中,事实上,因为没有对每个数据点进行多次实验,无
法减小随机过程的影响,因此结果会有各种变化,这也是上面图表保留了第六个世代的点
的原因,这时要提醒我们,神经网络的学习过程核心是随机过程,有时会工作得很好,有时
会很糟糕.
另一种可能是在世代数较大的情况下,学习率可能设置得过高了.将学习率调整为0.1,再
对比看看两种情况的性能:
</p>


<div class="figure">
<p><img src="imgs/00151.jpeg" alt="00151.jpeg" />
</p>
</div>

<p>
可以看到,在更多世代情况下,较小的学习率能得到更好的性能.
直观上,如果使用更长时间训练(探索梯度下降),就应使用更短的步长(学习率),那么就
能在总体上找到更好的路径.
</p>

<p>
看起来我们的神经网络甜蜜点是5个世代,但这种方法是相当不科学的,要正确科学的找
到甜蜜点,就应该为每个学习率和世代组合进行多次实验,尽量减少梯度下降过程中随机
性的影响.
</p>
</div>
</div>

<div id="outline-container-org3f87dd1" class="outline-4">
<h4 id="org3f87dd1">改变网络形状</h4>
<div class="outline-text-4" id="text-org3f87dd1">
<p>
我们还没尝试过改变神经网络的形状,也许我们早就该尝试了,之前一直使用的是100个
节点的隐藏层.
</p>

<p>
改变节点之前,我们思考下隐藏层,输入层和输出层都有什么作用,输入层只需要引入输
入信号,输出层只要送出神经网络的答案,是隐藏层(可以多层)进行学习,将输入变成,这
就是神经网络中"学习"过程发生的场所.事实上,隐藏层节点前后的链接权重承载了学习
能力.
</p>

<p>
计算机科学家将神经网络学习能力的限制称为 "学习容量",虽然学习能力不可能超过学
习容量,但可以通过增加学习容量来增加学习能力.链接权重具有学习能力,链接的总数
就代表"学习容量"了.类比人类感受冷热的过程:外部的冷热空气为输入信号,人体皮肤
上的感受器(以及相应的神经链接)为隐藏层,最终大脑皮层产生的冷热感受为输出新信
号;我们人体的皮肤上有温感受器和冷感受器,如果只有温感受器,当我们皮肤接触到冷
空气(输入信号),信号传递给温感受器,在温感受器这里信号会被抑制输出,因为它处理
不了这样的信号,最终我们就不会产生冷的感受(输出信号),我们就永远不会知道(学习)
冷是什么意思;但如果我们有针对多个温度的感受器呢?比如每隔一个摄氏度都有一种对
应的感受器,在一定的学习过程后,我们人类就可以不需要温度计直接感知外界温度了.
神经网络也是如此,那我们就试试增加隐藏层的节点,来看看性能如何:
</p>


<div class="figure">
<p><img src="imgs/00152.jpeg" alt="00152.jpeg" />
</p>
</div>

<p>
可以看到,较多的隐藏层节点效果确实比较少的好.随着节点数着呢个家,结果有改善,但
是不明显,由于增加一个节点意味着增加了节点前后层每隔节点的新网络,这以为这会产
生较多额外计算,因此,网络的训练时间也显著增加了,因此,必须在可以容忍的运行时间
内选择某个数目的节点.
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python"># neural network class definition
import matplotlib.pyplot
import numpy
from scipy.special import expit
class neuralNetwork:
    #initialize the neutral network.
    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):
	# set number of nodes in each input,hidden,output layer
	self.inodes = inputnodes
	self.hnodes = hiddennodes
	self.onodes = outputnodes
	# learning rate
	self.lr = learningrate
	self.wih = numpy.random.normal(0.0, pow(self.hnodes, -0.5), (self.hnodes, self.inodes))
	self.who = numpy.random.normal(0.0, pow(self.onodes, -0.5), (self.onodes, self.hnodes))
	self.activation_function = lambda x: expit(x)
	pass
    # train the neural network
    def train(self, input_list, targets_list):
	# convert input list to 2d array
	inputs = numpy.array(input_list, ndmin=2).T
	targets = numpy.array(targets_list, ndmin=2).T
	# calculate signals into hidden layer
	hidden_inputs = numpy.dot(self.wih, inputs)
	# calculate signals emerging from hidden layer
	hidden_outputs = self.activation_function(hidden_inputs)
	# calculate signals into final output layer
	final_inputs = numpy.dot(self.who, hidden_outputs)
	# calculate signals emerging from final output layer
	final_outputs = self.activation_function(final_inputs)

	# error is the (target - actual)
	output_errors = targets - final_outputs

	# hidden layer error is the output_errors, split by weights, recombined at hidden nodes
	hidden_errors = numpy.dot(self.who.T, output_errors)

	# update the weights for the links between the hidden and output layers
	self.who += self.lr * numpy.dot((output_errors * final_outputs * (1.0 - final_outputs)), numpy.transpose(hidden_outputs))
	# update the weights for the links between the hidden and output layers
	self.wih += self.lr * numpy.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), numpy.transpose(inputs))
	pass

    #query the neural network
    def query(self, inputs_list):
	# convert inputs list to 2d array
	inputs = numpy.array(inputs_list, ndmin=2).T
	# calculate signals into hidden layer
	hidden_inputs = numpy.dot(self.wih, inputs)
	# calculate the signals emerging form hidden layer
	hidden_outputs = self.activation_function(hidden_inputs)
	# calculate the signals into final output layer
	final_inputs = numpy.dot(self.who, hidden_outputs)
	# calculate the signals emerging from final output layer
	final_outputs = self.activation_function(final_inputs)
	return final_outputs

# number of input hidden and output nodes
input_nodes = 784
hidden_nodes = 500
output_nodes = 10

# learning rate is 0.3
learning_rate = 0.2

# create instance of neural network
n = neuralNetwork(input_nodes, hidden_nodes, output_nodes, learning_rate)

# load the mnist training data CSV file into a list
training_data_file = open("/home/kamisama/Downloads/mnist_train.csv", 'r')
training_data_list = training_data_file.readlines()
training_data_file.close()

# train the neural network
# go through all records in the training data set
# epochs is the number of times the training data set is used for training.
epochs = 2
for e in range(epochs):
    for record in training_data_list:
	# split the record by the ',' commas
	all_values = record.split(',')
	# scale and shift the inputs
	inputs = (numpy.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01
	# create the target output values (all 0.01, except the desired label which is 0.99)
	targets = numpy.zeros(output_nodes) + 0.01
	# all_values[0] is the target label for this record
	targets[int(all_values[0])] = 0.99
	n.train(inputs, targets)

# test the neural network
test_data_file = open("/home/kamisama/Downloads/mnist_test.csv", 'r')
test_data_list = test_data_file.readlines()
test_data_file.close()
# scorecard for how well the network performs, initially empty.
scorecard = []
# go through all the records in the test data set
for record in test_data_list:
    # split the record by the '.' commas
    all_values = record.split(',')
    # correct answer is first value
    correct_label = int(all_values[0])
    # print(correct_label, "correct label")
    # scale and shift the inputs
    inputs = (numpy.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01
    # query the network
    outputs = n.query(inputs)
    # the index of the highest value corresponds to the label
    label = numpy.argmax(outputs)
    # print(label, "network's answer")
    # append correct or in correct to list
    if (label == correct_label):
	# network's answer matches correct answer, add 1 to scorecard
	scorecard.append(1)
    else:
	# network's answer doesn't match correct answer, add 0 to scorecard
	scorecard.append(0)
	pass
    pass

# calculate the performance score, the fraction of correct answers
scorecard_array = numpy.asarray(scorecard)
print("performance = ", scorecard_array.sum()/scorecard_array.size)
</pre>
</div>

<pre class="example">
performance =  0.9663
</pre>


<p>
用我们并不严谨的方法测试出来甜蜜点处的学习率,世代,和隐藏层节点数进行训练,最
终得到的性能分数为0.9663,比优化前的约95%(0.9495)的准确度的成绩确实提高了不少.
注意,由于训练中的随机性,这些数据在每个人每一次的运行中都会有不同,不要纠结于
具体数字,重点是性能优化了.
</p>
</div>
</div>

<div id="outline-container-orge403155" class="outline-4">
<h4 id="orge403155">大功告成!</h4>
<div class="outline-text-4" id="text-orge403155">
<p>
回顾这一部分内容,我们只使用先前介绍的简单概念以及简单Python代码,就创建了一个
神经网络,没有任何多余花哨和神奇的数学,神经网络就已经有了不错的表现.
相比于学者和研究人员编写的神经网络,这个神经网络的表现也是可圈可点的(作者观
点,自身暂未接触到他人的成果.)
</p>
</div>
</div>


<div id="outline-container-orgc78bd80" class="outline-4">
<h4 id="orgc78bd80">最终代码:</h4>
<div class="outline-text-4" id="text-orgc78bd80">
<p>
由于网络资源访问的问题,中文版该书贴出了Github 上的到目前为止的最终代码,也就
是 <a href="#org3f87dd1">改变网络形状</a> 这一部分最后面的代码.
</p>
</div>
</div>
</div>
</div>



<div id="outline-container-org2e80dc4" class="outline-2">
<h2 id="org2e80dc4">趣味盎然</h2>
<div class="outline-text-2" id="text-org2e80dc4">
<p>
这一部分是一些有趣的想法,如果只是想了解神经网络的基本知识,到上一章最后就可以停
止阅读了.
</p>
</div>

<div id="outline-container-org1679c4f" class="outline-3">
<h3 id="org1679c4f">自己的手写数字</h3>
<div class="outline-text-3" id="text-org1679c4f">
<p>
上面我们使用的是MNIST数据集的数字图片,
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: kamisama</p>
</div>
</body>
</html>
